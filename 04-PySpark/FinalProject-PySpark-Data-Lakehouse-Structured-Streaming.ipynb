{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8476b62d-2825-400c-be4c-35bb3aa81b1c",
   "metadata": {},
   "source": [
    "## Lab 6: Building a Data Lakehouse with the PySpark Structured Streaming Medallion Architecture\n",
    "This lab will help you learn to use many of the software libraries and programming techniques required to fulfill the requirements of the final end-of-session capstone project for course **DS-2002: Data Systems**. The spirit of the project is to provide a capstone challenge that requires students to demonstrate a practical and functional understanding of each of the data systems and architectural principles covered throughout the session.\n",
    "\n",
    "**These include:**\n",
    "- Relational Database Management Systems (e.g., MySQL, Microsoft SQL Server, Oracle, IBM DB2)\n",
    "  - Online Transaction Processing Systems (OLTP): *Optimized for High-Volume Write Operations; Normalized to 3rd Normal Form.*\n",
    "  - Online Analytical Processing Systems (OLAP): *Optimized for Read/Aggregation Operations; Dimensional Model (i.e, Star Schema)*\n",
    "- NoSQL *(Not Only SQL)* Systems (e.g., MongoDB, CosmosDB, Cassandra, HBase, Redis)\n",
    "- File System *(Data Lake)* Source Systems (e.g., AWS S3, Microsoft Azure Data Lake Storage)\n",
    "  - Various Datafile Formats (e.g., JSON, CSV, Parquet, Text, Binary)\n",
    "- Massively Parallel Processing *(MPP)* Data Integration Systems (e.g., Apache Spark/PySpark, Databricks)\n",
    "- Data Integration Patterns (e.g., Extract-Transform-Load, Extract-Load-Transform, Extract-Load-Transform-Load, Lambda & Kappa Architectures)\n",
    "\n",
    "## Section I: Prerequisites\n",
    "\n",
    "### 1.0. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f57443f-8982-42df-9942-c72810e35969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymongo in /opt/anaconda3/envs/pysparkenv/lib/python3.12/site-packages (4.12.0)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /opt/anaconda3/envs/pysparkenv/lib/python3.12/site-packages (from pymongo) (2.7.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16355324-fc8a-45b5-b9e5-7b6e8ac814c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pysparkenv/lib/python3.12/site-packages/pyspark\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "print(findspark.find())\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import pymongo\n",
    "import certifi\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window as W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c2fdf8-2c35-4152-b60a-5e0ae632f60f",
   "metadata": {},
   "source": [
    "### 2.0. Instantiate Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "977209d2-77d8-40c6-a497-c5c0958c19c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Specify MySQL Server Connection Information\n",
    "# --------------------------------------------------------------------------------\n",
    "mysql_args = {\n",
    "    \"host_name\" : \"localhost\",\n",
    "    \"port\" : \"3306\",\n",
    "    \"db_name\" : \"northwind\",\n",
    "    \"conn_props\" : {\n",
    "        \"user\" : \"root\",\n",
    "        \"password\" : \"PASSWORD123!\",\n",
    "        \"driver\" : \"com.mysql.cj.jdbc.Driver\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Specify MongoDB Cluster Connection Information\n",
    "# --------------------------------------------------------------------------------\n",
    "mongodb_args = {\n",
    "    \"cluster_location\" : \"local\", # \"atlas\"\n",
    "    \"user_name\" : \"jtupitza\",\n",
    "    \"password\" : \"Passw0rd1234\",\n",
    "    \"cluster_name\" : \"sandbox\",\n",
    "    \"cluster_subnet\" : \"zibbf\",\n",
    "    \"db_name\" : \"northwind\",\n",
    "    \"collection\" : \"\",\n",
    "    \"null_column_threshold\" : 0.5\n",
    "}\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Specify Directory Structure for Source Data\n",
    "# --------------------------------------------------------------------------------\n",
    "base_dir = os.path.join(os.getcwd(), 'lab_data')\n",
    "data_dir = os.path.join(base_dir, 'northwind')\n",
    "batch_dir = os.path.join(data_dir, 'batch')\n",
    "stream_dir = os.path.join(data_dir, 'streaming')\n",
    "\n",
    "orders_stream_dir = os.path.join(stream_dir, 'orders')\n",
    "purchase_orders_stream_dir = os.path.join(stream_dir, 'purchase_orders')\n",
    "inventory_trans_stream_dir = os.path.join(stream_dir, 'inventory_transactions')\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Create Directory Structure for Data Lakehouse Files\n",
    "# --------------------------------------------------------------------------------\n",
    "dest_database = \"northwind_dlh\"\n",
    "sql_warehouse_dir = os.path.abspath('spark-warehouse')\n",
    "dest_database_dir = f\"{dest_database}.db\"\n",
    "database_dir = os.path.join(sql_warehouse_dir, dest_database_dir)\n",
    "\n",
    "orders_output_bronze = os.path.join(database_dir, 'fact_orders', 'bronze')\n",
    "orders_output_silver = os.path.join(database_dir, 'fact_orders', 'silver')\n",
    "orders_output_gold = os.path.join(database_dir, 'fact_orders', 'gold')\n",
    "\n",
    "purchase_orders_output_bronze = os.path.join(database_dir, 'fact_purchase_orders', 'bronze')\n",
    "purchase_orders_output_silver = os.path.join(database_dir, 'fact_purchase_orders', 'silver')\n",
    "purchase_orders_output_gold = os.path.join(database_dir, 'fact_purchase_orders', 'gold')\n",
    "\n",
    "inventory_trans_output_bronze = os.path.join(database_dir, 'fact_inventory_transactions', 'bronze')\n",
    "inventory_trans_output_silver = os.path.join(database_dir, 'fact_inventory_transactions', 'silver')\n",
    "inventory_trans_output_gold = os.path.join(database_dir, 'fact_inventory_transactions', 'gold')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021a5185-ad2c-4612-9498-0e2ff4431c5b",
   "metadata": {},
   "source": [
    "### 3.0. Define Global Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2fd4b85-6029-439d-a66a-1cab0a9aa760",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_info(path: str):\n",
    "    file_sizes = []\n",
    "    modification_times = []\n",
    "\n",
    "    '''Fetch each item in the directory, and filter out any directories.'''\n",
    "    items = os.listdir(path)\n",
    "    files = sorted([item for item in items if os.path.isfile(os.path.join(path, item))])\n",
    "\n",
    "    '''Populate lists with the Size and Last Modification DateTime for each file in the directory.'''\n",
    "    for file in files:\n",
    "        file_sizes.append(os.path.getsize(os.path.join(path, file)))\n",
    "        modification_times.append(pd.to_datetime(os.path.getmtime(os.path.join(path, file)), unit='s'))\n",
    "\n",
    "    data = list(zip(files, file_sizes, modification_times))\n",
    "    column_names = ['name','size','modification_time']\n",
    "    \n",
    "    return pd.DataFrame(data=data, columns=column_names)\n",
    "\n",
    "\n",
    "def wait_until_stream_is_ready(query, min_batches=1):\n",
    "    while len(query.recentProgress) < min_batches:\n",
    "        time.sleep(5)\n",
    "        \n",
    "    print(f\"The stream has processed {len(query.recentProgress)} batchs\")\n",
    "\n",
    "\n",
    "def remove_directory_tree(path: str):\n",
    "    '''If it exists, remove the entire contents of a directory structure at a given 'path' parameter's location.'''\n",
    "    try:\n",
    "        if os.path.exists(path):\n",
    "            shutil.rmtree(path)\n",
    "            return f\"Directory '{path}' has been removed successfully.\"\n",
    "        else:\n",
    "            return f\"Directory '{path}' does not exist.\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "        \n",
    "\n",
    "def drop_null_columns(df, threshold):\n",
    "    '''Drop Columns having a percentage of NULL values that exceeds the given 'threshold' parameter value.'''\n",
    "    columns_with_nulls = [col for col in df.columns if df.filter(df[col].isNull()).count() / df.count() > threshold] \n",
    "    df_dropped = df.drop(*columns_with_nulls) \n",
    "    \n",
    "    return df_dropped\n",
    "    \n",
    "    \n",
    "def get_mysql_dataframe(spark_session, sql_query : str, **args):\n",
    "    '''Create a JDBC URL to the MySQL Database'''\n",
    "    jdbc_url = f\"jdbc:mysql://{args['host_name']}:{args['port']}/{args['db_name']}\"\n",
    "    \n",
    "    '''Invoke the spark.read.format(\"jdbc\") function to query the database, and fill a DataFrame.'''\n",
    "    dframe = spark_session.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"driver\", args['conn_props']['driver']) \\\n",
    "    .option(\"user\", args['conn_props']['user']) \\\n",
    "    .option(\"password\", args['conn_props']['password']) \\\n",
    "    .option(\"query\", sql_query) \\\n",
    "    .load()\n",
    "    \n",
    "    return dframe\n",
    "    \n",
    "\n",
    "def get_mongo_uri(**args):\n",
    "    '''Validate proper input'''\n",
    "    if args[\"cluster_location\"] not in ['atlas', 'local']:\n",
    "        raise Exception(\"You must specify either 'atlas' or 'local' for the 'cluster_location' parameter.\")\n",
    "        \n",
    "    if args['cluster_location'] == \"atlas\":\n",
    "        uri = f\"mongodb+srv://{args['user_name']}:{args['password']}@\"\n",
    "        uri += f\"{args['cluster_name']}.{args['cluster_subnet']}.mongodb.net/\"\n",
    "    else:\n",
    "        uri = \"mongodb://localhost:27017/\"\n",
    "\n",
    "    return uri\n",
    "\n",
    "\n",
    "def get_spark_conf_args(spark_jars : list, **args):\n",
    "    jars = \"\"\n",
    "    for jar in spark_jars:\n",
    "        jars += f\"{jar}, \"\n",
    "    \n",
    "    sparkConf_args = {\n",
    "        \"app_name\" : \"PySpark Northwind Data Lakehouse (Medallion Architecture)\",\n",
    "        \"worker_threads\" : f\"local[{int(os.cpu_count()/2)}]\",\n",
    "        \"shuffle_partitions\" : int(os.cpu_count()),\n",
    "        \"mongo_uri\" : get_mongo_uri(**args),\n",
    "        \"spark_jars\" : jars[0:-2],\n",
    "        \"database_dir\" : sql_warehouse_dir\n",
    "    }\n",
    "    \n",
    "    return sparkConf_args\n",
    "    \n",
    "\n",
    "def get_spark_conf(**args):\n",
    "    sparkConf = SparkConf().setAppName(args['app_name'])\\\n",
    "    .setMaster(args['worker_threads']) \\\n",
    "    .set('spark.driver.memory', '8g') \\\n",
    "    .set('spark.executor.memory', '4g') \\\n",
    "    .set('spark.jars', args['spark_jars']) \\\n",
    "    .set('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1') \\\n",
    "    .set('spark.mongodb.input.uri', args['mongo_uri']) \\\n",
    "    .set('spark.mongodb.output.uri', args['mongo_uri']) \\\n",
    "    .set('spark.sql.adaptive.enabled', 'false') \\\n",
    "    .set('spark.sql.debug.maxToStringFields', 35) \\\n",
    "    .set('spark.sql.shuffle.partitions', args['shuffle_partitions']) \\\n",
    "    .set('spark.sql.streaming.forceDeleteTempCheckpointLocation', 'true') \\\n",
    "    .set('spark.sql.streaming.schemaInference', 'true') \\\n",
    "    .set('spark.sql.warehouse.dir', args['database_dir']) \\\n",
    "    .set('spark.streaming.stopGracefullyOnShutdown', 'true')\n",
    "    \n",
    "    return sparkConf\n",
    "\n",
    "\n",
    "def get_mongo_client(**args):\n",
    "    '''Get MongoDB Client Connection'''\n",
    "    mongo_uri = get_mongo_uri(**args)\n",
    "    if args['cluster_location'] == \"atlas\":\n",
    "        client = pymongo.MongoClient(mongo_uri, tlsCAFile=certifi.where())\n",
    "\n",
    "    elif args['cluster_location'] == \"local\":\n",
    "        client = pymongo.MongoClient(mongo_uri)\n",
    "        \n",
    "    else:\n",
    "        raise Exception(\"A MongoDB Client could not be created.\")\n",
    "\n",
    "    return client\n",
    "    \n",
    "    \n",
    "# TODO: Rewrite this to leverage PySpark?\n",
    "def set_mongo_collections(mongo_client, db_name : str, data_directory : str, json_files : list):\n",
    "    db = mongo_client[db_name]\n",
    "    \n",
    "    for file in json_files:\n",
    "        db.drop_collection(file)\n",
    "        json_file = os.path.join(data_directory, json_files[file])\n",
    "        with open(json_file, 'r') as openfile:\n",
    "            json_object = json.load(openfile)\n",
    "            file = db[file]\n",
    "            result = file.insert_many(json_object)\n",
    "        \n",
    "    mongo_client.close()\n",
    "    \n",
    "\n",
    "def get_mongodb_dataframe(spark_session, **args):\n",
    "    '''Query MongoDB, and create a DataFrame'''\n",
    "    dframe = spark_session.read.format(\"com.mongodb.spark.sql.DefaultSource\") \\\n",
    "        .option(\"database\", args['db_name']) \\\n",
    "        .option(\"collection\", args['collection']).load()\n",
    "\n",
    "    '''Drop the '_id' index column to clean up the response.'''\n",
    "    dframe = dframe.drop('_id')\n",
    "    \n",
    "    '''Call the drop_null_columns() function passing in the dataframe.'''\n",
    "    dframe = drop_null_columns(dframe, args['null_column_threshold'])\n",
    "    \n",
    "    return dframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c929bad4-705f-4a01-8d9d-bba2d84d115c",
   "metadata": {},
   "source": [
    "### 4.0. Initialize Data Lakehouse Directory Structure\n",
    "Remove the Data Lakehouse Database Directory Structure to Ensure Idempotency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c09080f-afdc-4969-86cc-0fbd0835faf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Directory '/Users/sonika/Desktop/DS 2002/DS-2002/04-PySpark/spark-warehouse/northwind_dlh.db' has been removed successfully.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_directory_tree(database_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa65216-97fc-48d9-b19e-a1f342c53155",
   "metadata": {},
   "source": [
    "### 5.0. Create a New Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a416acc5-71ca-40f5-986a-7eeac55b6317",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/21 17:26:02 WARN Utils: Your hostname, Sonikas-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.1.63 instead (on interface en0)\n",
      "25/04/21 17:26:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /Users/sonika/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/sonika/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-487cb94c-d57f-4a78-a031-dfdfc8e70ddb;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      ":: resolution report :: resolve 72ms :: artifacts dl 3ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-487cb94c-d57f-4a78-a031-dfdfc8e70ddb\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/2ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/anaconda3/envs/pysparkenv/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/21 17:26:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.63:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySpark Northwind Data Lakehouse (Medallion Architecture)</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x14e8834a0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worker_threads = f\"local[{int(os.cpu_count()/2)}]\"\n",
    "\n",
    "jars = []\n",
    "mysql_spark_jar = os.path.join(os.getcwd(), \"mysql-connector-j-9.1.0\", \"mysql-connector-j-9.1.0.jar\")\n",
    "mssql_spark_jar = os.path.join(os.getcwd(), \"sqljdbc_12.8\", \"enu\", \"jars\", \"mssql-jdbc-12.8.1.jre11.jar\")\n",
    "\n",
    "jars.append(mysql_spark_jar)\n",
    "#jars.append(mssql_spark_jar)\n",
    "\n",
    "sparkConf_args = get_spark_conf_args(jars, **mongodb_args)\n",
    "\n",
    "sparkConf = get_spark_conf(**sparkConf_args)\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"OFF\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e8701c-0120-4ba2-81cc-e53ba1f08651",
   "metadata": {},
   "source": [
    "### 6.0. Create a New Metadata Database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "484d76e2-3c95-4d0e-9063-37344aae674c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"DROP DATABASE IF EXISTS {dest_database} CASCADE;\")\n",
    "\n",
    "sql_create_db = f\"\"\"\n",
    "    CREATE DATABASE IF NOT EXISTS {dest_database}\n",
    "    COMMENT 'DS-2002 Lab 06 Database'\n",
    "    WITH DBPROPERTIES (contains_pii = true, purpose = 'DS-2002 Lab 6.0');\n",
    "\"\"\"\n",
    "spark.sql(sql_create_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ae5f1c-7ec7-408c-bc89-822b7a75b859",
   "metadata": {},
   "source": [
    "## Section II: Populate Dimensions by Ingesting \"Cold-path\" Reference Data \n",
    "### 1.0. Fetch Data from the File System\n",
    "#### 1.1. Verify the location of the source data files on the file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a3629bc-e7ec-4ed4-9de7-17f648691e4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>size</th>\n",
       "      <th>modification_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>northwind_customers.json</td>\n",
       "      <td>10186</td>\n",
       "      <td>2025-04-01 16:28:26.273118019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>northwind_employees.csv</td>\n",
       "      <td>2687</td>\n",
       "      <td>2025-04-01 16:28:26.273163795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>northwind_invoices.json</td>\n",
       "      <td>5843</td>\n",
       "      <td>2025-04-01 16:28:26.273231030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>northwind_shippers.csv</td>\n",
       "      <td>253</td>\n",
       "      <td>2025-04-01 16:28:26.273276329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>northwind_suppliers.json</td>\n",
       "      <td>1380</td>\n",
       "      <td>2025-04-01 16:28:26.273316860</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       name   size             modification_time\n",
       "0  northwind_customers.json  10186 2025-04-01 16:28:26.273118019\n",
       "1   northwind_employees.csv   2687 2025-04-01 16:28:26.273163795\n",
       "2   northwind_invoices.json   5843 2025-04-01 16:28:26.273231030\n",
       "3    northwind_shippers.csv    253 2025-04-01 16:28:26.273276329\n",
       "4  northwind_suppliers.json   1380 2025-04-01 16:28:26.273316860"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_file_info(batch_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e13a92a-2e5e-46b3-babc-ac10f20d35da",
   "metadata": {},
   "source": [
    "#### 1.2. Populate the <span style=\"color:darkred\">Employees Dimension</span>\n",
    "##### 1.2.1. Use PySpark to Read data from a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63f9eac0-5c92-4a98-8455-8be2a034d935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sonika/Desktop/DS 2002/DS-2002/04-PySpark/lab_data/northwind/batch/northwind_employees.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>company</th>\n",
       "      <th>last_name</th>\n",
       "      <th>first_name</th>\n",
       "      <th>email_address</th>\n",
       "      <th>job_title</th>\n",
       "      <th>business_phone</th>\n",
       "      <th>home_phone</th>\n",
       "      <th>mobile_phone</th>\n",
       "      <th>fax_number</th>\n",
       "      <th>address</th>\n",
       "      <th>city</th>\n",
       "      <th>state_province</th>\n",
       "      <th>zip_postal_code</th>\n",
       "      <th>country_region</th>\n",
       "      <th>web_page</th>\n",
       "      <th>notes</th>\n",
       "      <th>attachments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Northwind Traders</td>\n",
       "      <td>Freehafer</td>\n",
       "      <td>Nancy</td>\n",
       "      <td>nancy@northwindtraders.com</td>\n",
       "      <td>Sales Representative</td>\n",
       "      <td>(123)555-0100</td>\n",
       "      <td>(123)555-0102</td>\n",
       "      <td>NULL</td>\n",
       "      <td>(123)555-0103</td>\n",
       "      <td>123 1st Avenue</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>99999</td>\n",
       "      <td>USA</td>\n",
       "      <td>#http://northwindtraders.com#</td>\n",
       "      <td>NULL</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Northwind Traders</td>\n",
       "      <td>Cencini</td>\n",
       "      <td>Andrew</td>\n",
       "      <td>andrew@northwindtraders.com</td>\n",
       "      <td>Vice President, Sales</td>\n",
       "      <td>(123)555-0100</td>\n",
       "      <td>(123)555-0102</td>\n",
       "      <td>NULL</td>\n",
       "      <td>(123)555-0103</td>\n",
       "      <td>123 2nd Avenue</td>\n",
       "      <td>Bellevue</td>\n",
       "      <td>WA</td>\n",
       "      <td>99999</td>\n",
       "      <td>USA</td>\n",
       "      <td>http://northwindtraders.com#http://northwindtr...</td>\n",
       "      <td>Joined the company as a sales representative, ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id            company  last_name first_name                email_address  \\\n",
       "0   1  Northwind Traders  Freehafer      Nancy   nancy@northwindtraders.com   \n",
       "1   2  Northwind Traders    Cencini     Andrew  andrew@northwindtraders.com   \n",
       "\n",
       "               job_title business_phone     home_phone mobile_phone  \\\n",
       "0   Sales Representative  (123)555-0100  (123)555-0102         NULL   \n",
       "1  Vice President, Sales  (123)555-0100  (123)555-0102         NULL   \n",
       "\n",
       "      fax_number         address      city state_province  zip_postal_code  \\\n",
       "0  (123)555-0103  123 1st Avenue   Seattle             WA            99999   \n",
       "1  (123)555-0103  123 2nd Avenue  Bellevue             WA            99999   \n",
       "\n",
       "  country_region                                           web_page  \\\n",
       "0            USA                      #http://northwindtraders.com#   \n",
       "1            USA  http://northwindtraders.com#http://northwindtr...   \n",
       "\n",
       "                                               notes attachments  \n",
       "0                                               NULL        None  \n",
       "1  Joined the company as a sales representative, ...        None  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employee_csv = os.path.join(batch_dir, 'northwind_employees.csv')\n",
    "print(employee_csv)\n",
    "\n",
    "df_dim_employees = spark.read.format('csv').options(header='true', inferSchema='true').load(employee_csv)\n",
    "df_dim_employees.toPandas().head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e9aedf-9033-403d-a802-b09be878865f",
   "metadata": {},
   "source": [
    "##### 1.2.2. Make Necessary Transformations to the New DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18421cba-8e15-44b0-8328-0ff1f1968936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>employee_key</th>\n",
       "      <th>employee_id</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>company</th>\n",
       "      <th>job_title</th>\n",
       "      <th>business_phone</th>\n",
       "      <th>home_phone</th>\n",
       "      <th>fax_number</th>\n",
       "      <th>address</th>\n",
       "      <th>city</th>\n",
       "      <th>state_province</th>\n",
       "      <th>zip_postal_code</th>\n",
       "      <th>country_region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Nancy</td>\n",
       "      <td>Freehafer</td>\n",
       "      <td>Northwind Traders</td>\n",
       "      <td>Sales Representative</td>\n",
       "      <td>(123)555-0100</td>\n",
       "      <td>(123)555-0102</td>\n",
       "      <td>(123)555-0103</td>\n",
       "      <td>123 1st Avenue</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>99999</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Andrew</td>\n",
       "      <td>Cencini</td>\n",
       "      <td>Northwind Traders</td>\n",
       "      <td>Vice President, Sales</td>\n",
       "      <td>(123)555-0100</td>\n",
       "      <td>(123)555-0102</td>\n",
       "      <td>(123)555-0103</td>\n",
       "      <td>123 2nd Avenue</td>\n",
       "      <td>Bellevue</td>\n",
       "      <td>WA</td>\n",
       "      <td>99999</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   employee_key  employee_id first_name  last_name            company  \\\n",
       "0             1            1      Nancy  Freehafer  Northwind Traders   \n",
       "1             2            2     Andrew    Cencini  Northwind Traders   \n",
       "\n",
       "               job_title business_phone     home_phone     fax_number  \\\n",
       "0   Sales Representative  (123)555-0100  (123)555-0102  (123)555-0103   \n",
       "1  Vice President, Sales  (123)555-0100  (123)555-0102  (123)555-0103   \n",
       "\n",
       "          address      city state_province  zip_postal_code country_region  \n",
       "0  123 1st Avenue   Seattle             WA            99999            USA  \n",
       "1  123 2nd Avenue  Bellevue             WA            99999            USA  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------\n",
    "# Rename the 'id' column to 'employee_id' ------------------------------------------\n",
    "# ----------------------------------------------------------------------------------\n",
    "df_dim_employees = df_dim_employees.withColumnRenamed(\"id\", \"employee_id\")\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Add Primary Key column using SQL Windowing function: ROW_NUMBER() \n",
    "# ----------------------------------------------------------------------------------\n",
    "df_dim_employees.createOrReplaceTempView(\"employees\")\n",
    "sql_employees = f\"\"\"\n",
    "    SELECT *, ROW_NUMBER() OVER (ORDER BY employee_id) AS employee_key\n",
    "    FROM employees;\n",
    "\"\"\"\n",
    "df_dim_employees = spark.sql(sql_employees)\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Reorder Columns and display the first two rows in a Pandas dataframe\n",
    "# ----------------------------------------------------------------------------------\n",
    "ordered_columns = ['employee_key', 'employee_id', 'first_name', 'last_name'\n",
    "                   , 'company', 'job_title', 'business_phone', 'home_phone', 'fax_number'\n",
    "                   , 'address', 'city', 'state_province', 'zip_postal_code', 'country_region']\n",
    "\n",
    "df_dim_employees = df_dim_employees[ordered_columns]\n",
    "df_dim_employees.toPandas().head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043a826b-085a-42ce-b9b1-f0991e578eb6",
   "metadata": {},
   "source": [
    "##### 1.2.3. Save as the <span style=\"color:darkred\">dim_employees</span> table in the Data Lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5931ed64-99f7-4281-b903-a052c89ce6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_employees.write.saveAsTable(f\"{dest_database}.dim_employees\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7eacbe-1cad-4ded-9b98-814d140ed8d6",
   "metadata": {},
   "source": [
    "##### 1.2.4. Unit Test: Describe and Preview Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "414f418e-b9aa-4ef0-a256-b7dd3de1519b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|        employee_key|                 int|   NULL|\n",
      "|         employee_id|                 int|   NULL|\n",
      "|          first_name|              string|   NULL|\n",
      "|           last_name|              string|   NULL|\n",
      "|             company|              string|   NULL|\n",
      "|           job_title|              string|   NULL|\n",
      "|      business_phone|              string|   NULL|\n",
      "|          home_phone|              string|   NULL|\n",
      "|          fax_number|              string|   NULL|\n",
      "|             address|              string|   NULL|\n",
      "|                city|              string|   NULL|\n",
      "|      state_province|              string|   NULL|\n",
      "|     zip_postal_code|                 int|   NULL|\n",
      "|      country_region|              string|   NULL|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|             Catalog|       spark_catalog|       |\n",
      "|            Database|       northwind_dlh|       |\n",
      "|               Table|       dim_employees|       |\n",
      "|        Created Time|Mon Apr 21 17:26:...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>employee_key</th>\n",
       "      <th>employee_id</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>company</th>\n",
       "      <th>job_title</th>\n",
       "      <th>business_phone</th>\n",
       "      <th>home_phone</th>\n",
       "      <th>fax_number</th>\n",
       "      <th>address</th>\n",
       "      <th>city</th>\n",
       "      <th>state_province</th>\n",
       "      <th>zip_postal_code</th>\n",
       "      <th>country_region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Nancy</td>\n",
       "      <td>Freehafer</td>\n",
       "      <td>Northwind Traders</td>\n",
       "      <td>Sales Representative</td>\n",
       "      <td>(123)555-0100</td>\n",
       "      <td>(123)555-0102</td>\n",
       "      <td>(123)555-0103</td>\n",
       "      <td>123 1st Avenue</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>99999</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Andrew</td>\n",
       "      <td>Cencini</td>\n",
       "      <td>Northwind Traders</td>\n",
       "      <td>Vice President, Sales</td>\n",
       "      <td>(123)555-0100</td>\n",
       "      <td>(123)555-0102</td>\n",
       "      <td>(123)555-0103</td>\n",
       "      <td>123 2nd Avenue</td>\n",
       "      <td>Bellevue</td>\n",
       "      <td>WA</td>\n",
       "      <td>99999</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   employee_key  employee_id first_name  last_name            company  \\\n",
       "0             1            1      Nancy  Freehafer  Northwind Traders   \n",
       "1             2            2     Andrew    Cencini  Northwind Traders   \n",
       "\n",
       "               job_title business_phone     home_phone     fax_number  \\\n",
       "0   Sales Representative  (123)555-0100  (123)555-0102  (123)555-0103   \n",
       "1  Vice President, Sales  (123)555-0100  (123)555-0102  (123)555-0103   \n",
       "\n",
       "          address      city state_province  zip_postal_code country_region  \n",
       "0  123 1st Avenue   Seattle             WA            99999            USA  \n",
       "1  123 2nd Avenue  Bellevue             WA            99999            USA  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"DESCRIBE EXTENDED {dest_database}.dim_employees;\").show()\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.dim_employees LIMIT 2\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebca6e1-cf05-44bc-b2c4-29ea6492d5c3",
   "metadata": {},
   "source": [
    "#### 1.3. Populate the <span style=\"color:darkred\">Shippers Dimension</span>\n",
    "##### 1.3.1. Use PySpark to Read Data from a CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "983f5587-5c93-409f-ba2d-d60883a82c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>company</th>\n",
       "      <th>address</th>\n",
       "      <th>city</th>\n",
       "      <th>state_province</th>\n",
       "      <th>zip_postal_code</th>\n",
       "      <th>country_region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Shipping Company A</td>\n",
       "      <td>123 Any Street</td>\n",
       "      <td>Memphis</td>\n",
       "      <td>TN</td>\n",
       "      <td>99999</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Shipping Company B</td>\n",
       "      <td>123 Any Street</td>\n",
       "      <td>Memphis</td>\n",
       "      <td>TN</td>\n",
       "      <td>99999</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id             company         address     city state_province  \\\n",
       "0   1  Shipping Company A  123 Any Street  Memphis             TN   \n",
       "1   2  Shipping Company B  123 Any Street  Memphis             TN   \n",
       "\n",
       "   zip_postal_code country_region  \n",
       "0            99999            USA  \n",
       "1            99999            USA  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1). Get a reference to the 'northwind_shippers.csv' file.\n",
    "shipper_csv = os.path.join(batch_dir, 'northwind_shippers.csv')\n",
    "\n",
    "# 2). Use Spark to read the CSV file data into the 'df_dim_shippers' variable.\n",
    "#     Remember to specify that the first row contains column names (header), and to infer the schema.\n",
    "df_dim_shippers = spark.read.format('csv').options(header='true', inferSchema='true').load(shipper_csv)\n",
    "\n",
    "# 3). Unit Test: Convert the spark dataframe to a Pandas dataframe, and display the first two rows.\n",
    "df_dim_shippers.toPandas().head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ce5dcc-59df-4f7e-abf2-604668f77ed4",
   "metadata": {},
   "source": [
    "##### 1.3.2 Make Necessary Transformations to the New DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7498239a-2320-4496-b594-18fb5fe6279d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shipper_key</th>\n",
       "      <th>shipper_id</th>\n",
       "      <th>company</th>\n",
       "      <th>address</th>\n",
       "      <th>city</th>\n",
       "      <th>state_province</th>\n",
       "      <th>zip_postal_code</th>\n",
       "      <th>country_region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Shipping Company A</td>\n",
       "      <td>123 Any Street</td>\n",
       "      <td>Memphis</td>\n",
       "      <td>TN</td>\n",
       "      <td>99999</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Shipping Company B</td>\n",
       "      <td>123 Any Street</td>\n",
       "      <td>Memphis</td>\n",
       "      <td>TN</td>\n",
       "      <td>99999</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   shipper_key  shipper_id             company         address     city  \\\n",
       "0            1           1  Shipping Company A  123 Any Street  Memphis   \n",
       "1            2           2  Shipping Company B  123 Any Street  Memphis   \n",
       "\n",
       "  state_province  zip_postal_code country_region  \n",
       "0             TN            99999            USA  \n",
       "1             TN            99999            USA  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------\n",
    "# Rename the 'id' column to 'shipper_id' ------------------------------------------\n",
    "# ----------------------------------------------------------------------------------\n",
    "df_dim_shippers = df_dim_shippers.withColumnRenamed(\"id\", \"shipper_id\")\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Add Primary Key column using SQL Windowing function: ROW_NUMBER() \n",
    "# ----------------------------------------------------------------------------------\n",
    "df_dim_shippers.createOrReplaceTempView(\"shippers\")\n",
    "sql_shippers = f\"\"\"\n",
    "    SELECT *, ROW_NUMBER() OVER (ORDER BY shipper_id) AS shipper_key\n",
    "    FROM shippers;\n",
    "\"\"\"\n",
    "df_dim_shippers = spark.sql(sql_shippers)\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Reorder Columns and display the first two rows in a Pandas dataframe\n",
    "# ----------------------------------------------------------------------------------\n",
    "ordered_columns = ['shipper_key', 'shipper_id', 'company','address','city','state_province','zip_postal_code','country_region']\n",
    "\n",
    "df_dim_shippers = df_dim_shippers[ordered_columns]\n",
    "df_dim_shippers.toPandas().head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958c325c-2fe6-4c8e-b8c2-aeefb127c3ab",
   "metadata": {},
   "source": [
    "##### 1.3.3. Save as the <span style=\"color:darkred\">dim_shippers</span> table in the Data Lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d688f917-d9df-4bee-bd94-1a6147a5558b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_shippers.write.saveAsTable(f\"{dest_database}.dim_shippers\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cb59e9-b2a7-4cd7-b601-9e99857af89e",
   "metadata": {},
   "source": [
    "##### 1.3.4. Unit Test: Describe and Preview Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e73177f0-7590-491e-ae7e-d5256a11dc33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|         shipper_key|                 int|   NULL|\n",
      "|          shipper_id|                 int|   NULL|\n",
      "|             company|              string|   NULL|\n",
      "|             address|              string|   NULL|\n",
      "|                city|              string|   NULL|\n",
      "|      state_province|              string|   NULL|\n",
      "|     zip_postal_code|                 int|   NULL|\n",
      "|      country_region|              string|   NULL|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|             Catalog|       spark_catalog|       |\n",
      "|            Database|       northwind_dlh|       |\n",
      "|               Table|        dim_shippers|       |\n",
      "|        Created Time|Mon Apr 21 17:26:...|       |\n",
      "|         Last Access|             UNKNOWN|       |\n",
      "|          Created By|         Spark 3.5.5|       |\n",
      "|                Type|             MANAGED|       |\n",
      "|            Provider|             parquet|       |\n",
      "|            Location|file:/Users/sonik...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shipper_key</th>\n",
       "      <th>shipper_id</th>\n",
       "      <th>company</th>\n",
       "      <th>address</th>\n",
       "      <th>city</th>\n",
       "      <th>state_province</th>\n",
       "      <th>zip_postal_code</th>\n",
       "      <th>country_region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Shipping Company A</td>\n",
       "      <td>123 Any Street</td>\n",
       "      <td>Memphis</td>\n",
       "      <td>TN</td>\n",
       "      <td>99999</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Shipping Company B</td>\n",
       "      <td>123 Any Street</td>\n",
       "      <td>Memphis</td>\n",
       "      <td>TN</td>\n",
       "      <td>99999</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   shipper_key  shipper_id             company         address     city  \\\n",
       "0            1           1  Shipping Company A  123 Any Street  Memphis   \n",
       "1            2           2  Shipping Company B  123 Any Street  Memphis   \n",
       "\n",
       "  state_province  zip_postal_code country_region  \n",
       "0             TN            99999            USA  \n",
       "1             TN            99999            USA  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"DESCRIBE EXTENDED {dest_database}.dim_shippers;\").show()\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.dim_shippers LIMIT 2\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c1854d-8ea5-421d-b2d7-62109d67581f",
   "metadata": {},
   "source": [
    "### 2.0. Fetch Reference Data from a MongoDB Atlas Database\n",
    "#### 2.1. Create a New MongoDB Database, and Load Each JSON File into a New MongoDB Collection\n",
    "**NOTE:** The following cell **can** be run more than once because the **set_mongo_collection()** function **is** idempotent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f14ae8b9-128b-49c6-9aeb-532dafdafb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = get_mongo_client(**mongodb_args)\n",
    "\n",
    "json_files = {\"customers\" : \"northwind_customers.json\",\n",
    "              \"invoices\" : 'northwind_invoices.json',\n",
    "              \"suppliers\" : 'northwind_suppliers.json'\n",
    "             }\n",
    "\n",
    "set_mongo_collections(client, mongodb_args[\"db_name\"], batch_dir, json_files) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1565e612-daeb-45f0-a743-e05b3a137c6e",
   "metadata": {},
   "source": [
    "#### 2.2. Populate the <span style=\"color:darkred\">Customers Dimension</span>\n",
    "##### 2.2.1. Fetch Data from the New MongoDB <span style=\"color:darkred\">Customers</span> Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e2b966f7-d6e4-4f51-81b8-871099605278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>address</th>\n",
       "      <th>business_phone</th>\n",
       "      <th>city</th>\n",
       "      <th>company</th>\n",
       "      <th>country_region</th>\n",
       "      <th>fax_number</th>\n",
       "      <th>first_name</th>\n",
       "      <th>id</th>\n",
       "      <th>job_title</th>\n",
       "      <th>last_name</th>\n",
       "      <th>state_province</th>\n",
       "      <th>zip_postal_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123 1st Street</td>\n",
       "      <td>(123)555-0100</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>Company A</td>\n",
       "      <td>USA</td>\n",
       "      <td>(123)555-0101</td>\n",
       "      <td>Anna</td>\n",
       "      <td>1</td>\n",
       "      <td>Owner</td>\n",
       "      <td>Bedecs</td>\n",
       "      <td>WA</td>\n",
       "      <td>99999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>123 2nd Street</td>\n",
       "      <td>(123)555-0100</td>\n",
       "      <td>Boston</td>\n",
       "      <td>Company B</td>\n",
       "      <td>USA</td>\n",
       "      <td>(123)555-0101</td>\n",
       "      <td>Antonio</td>\n",
       "      <td>2</td>\n",
       "      <td>Owner</td>\n",
       "      <td>Gratacos Solsona</td>\n",
       "      <td>MA</td>\n",
       "      <td>99999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          address business_phone     city    company country_region  \\\n",
       "0  123 1st Street  (123)555-0100  Seattle  Company A            USA   \n",
       "1  123 2nd Street  (123)555-0100   Boston  Company B            USA   \n",
       "\n",
       "      fax_number first_name  id job_title         last_name state_province  \\\n",
       "0  (123)555-0101       Anna   1     Owner            Bedecs             WA   \n",
       "1  (123)555-0101    Antonio   2     Owner  Gratacos Solsona             MA   \n",
       "\n",
       "  zip_postal_code  \n",
       "0           99999  \n",
       "1           99999  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mongodb_args[\"collection\"] = \"customers\"\n",
    "\n",
    "df_dim_customers = get_mongodb_dataframe(spark, **mongodb_args)\n",
    "df_dim_customers.toPandas().head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59029949-2103-40b8-8393-3707e0e83846",
   "metadata": {},
   "source": [
    "##### 2.2.2. Make Necessary Transformations to the New Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "30819428-fcf7-4668-b080-b124da524417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_key</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>company</th>\n",
       "      <th>job_title</th>\n",
       "      <th>business_phone</th>\n",
       "      <th>fax_number</th>\n",
       "      <th>address</th>\n",
       "      <th>city</th>\n",
       "      <th>state_province</th>\n",
       "      <th>zip_postal_code</th>\n",
       "      <th>country_region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Anna</td>\n",
       "      <td>Bedecs</td>\n",
       "      <td>Company A</td>\n",
       "      <td>Owner</td>\n",
       "      <td>(123)555-0100</td>\n",
       "      <td>(123)555-0101</td>\n",
       "      <td>123 1st Street</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>99999</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Antonio</td>\n",
       "      <td>Gratacos Solsona</td>\n",
       "      <td>Company B</td>\n",
       "      <td>Owner</td>\n",
       "      <td>(123)555-0100</td>\n",
       "      <td>(123)555-0101</td>\n",
       "      <td>123 2nd Street</td>\n",
       "      <td>Boston</td>\n",
       "      <td>MA</td>\n",
       "      <td>99999</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_key  customer_id first_name         last_name    company  \\\n",
       "0             1            1       Anna            Bedecs  Company A   \n",
       "1             2            2    Antonio  Gratacos Solsona  Company B   \n",
       "\n",
       "  job_title business_phone     fax_number         address     city  \\\n",
       "0     Owner  (123)555-0100  (123)555-0101  123 1st Street  Seattle   \n",
       "1     Owner  (123)555-0100  (123)555-0101  123 2nd Street   Boston   \n",
       "\n",
       "  state_province zip_postal_code country_region  \n",
       "0             WA           99999            USA  \n",
       "1             MA           99999            USA  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------\n",
    "# Rename the 'id' column to 'customer_id' ------------------------------------------\n",
    "# ----------------------------------------------------------------------------------\n",
    "df_dim_customers = df_dim_customers.withColumnRenamed(\"id\", \"customer_id\")\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Add Primary Key column using the SQL Windowing function: ROW_NUMBER() \n",
    "# ----------------------------------------------------------------------------------\n",
    "df_dim_customers.createOrReplaceTempView(\"customers\")\n",
    "sql_customers = f\"\"\"\n",
    "    SELECT *, ROW_NUMBER() OVER (ORDER BY customer_id) AS customer_key\n",
    "    FROM customers;\n",
    "\"\"\"\n",
    "df_dim_customers = spark.sql(sql_customers)\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Reorder Columns and display the first two rows in a Pandas dataframe\n",
    "# ----------------------------------------------------------------------------------\n",
    "ordered_columns = ['customer_key','customer_id','first_name','last_name','company','job_title','business_phone','fax_number', 'address','city','state_province','zip_postal_code','country_region']\n",
    "df_dim_customers = df_dim_customers[ordered_columns]\n",
    "df_dim_customers.toPandas().head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c71d4b7-b620-49db-ba43-0b5c83b77b61",
   "metadata": {},
   "source": [
    "##### 2.2.3. Save as the <span style=\"color:darkred\">dim_customers</span> table in the Data lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d3467a6d-8037-4d25-b9c1-18e9bbc413c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_customers.write.saveAsTable(f\"{dest_database}.dim_customers\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d8d82a-dba4-40e3-aecc-0f4763b94679",
   "metadata": {},
   "source": [
    "##### 2.2.4. Unit Test: Describe and Preview Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "64add729-9a97-483c-874e-c5aede8eadc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|        customer_key|                 int|   NULL|\n",
      "|         customer_id|                 int|   NULL|\n",
      "|          first_name|              string|   NULL|\n",
      "|           last_name|              string|   NULL|\n",
      "|             company|              string|   NULL|\n",
      "|           job_title|              string|   NULL|\n",
      "|      business_phone|              string|   NULL|\n",
      "|          fax_number|              string|   NULL|\n",
      "|             address|              string|   NULL|\n",
      "|                city|              string|   NULL|\n",
      "|      state_province|              string|   NULL|\n",
      "|     zip_postal_code|              string|   NULL|\n",
      "|      country_region|              string|   NULL|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|             Catalog|       spark_catalog|       |\n",
      "|            Database|       northwind_dlh|       |\n",
      "|               Table|       dim_customers|       |\n",
      "|        Created Time|Mon Apr 21 17:26:...|       |\n",
      "|         Last Access|             UNKNOWN|       |\n",
      "+--------------------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_key</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>company</th>\n",
       "      <th>job_title</th>\n",
       "      <th>business_phone</th>\n",
       "      <th>fax_number</th>\n",
       "      <th>address</th>\n",
       "      <th>city</th>\n",
       "      <th>state_province</th>\n",
       "      <th>zip_postal_code</th>\n",
       "      <th>country_region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Anna</td>\n",
       "      <td>Bedecs</td>\n",
       "      <td>Company A</td>\n",
       "      <td>Owner</td>\n",
       "      <td>(123)555-0100</td>\n",
       "      <td>(123)555-0101</td>\n",
       "      <td>123 1st Street</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>99999</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Antonio</td>\n",
       "      <td>Gratacos Solsona</td>\n",
       "      <td>Company B</td>\n",
       "      <td>Owner</td>\n",
       "      <td>(123)555-0100</td>\n",
       "      <td>(123)555-0101</td>\n",
       "      <td>123 2nd Street</td>\n",
       "      <td>Boston</td>\n",
       "      <td>MA</td>\n",
       "      <td>99999</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_key  customer_id first_name         last_name    company  \\\n",
       "0             1            1       Anna            Bedecs  Company A   \n",
       "1             2            2    Antonio  Gratacos Solsona  Company B   \n",
       "\n",
       "  job_title business_phone     fax_number         address     city  \\\n",
       "0     Owner  (123)555-0100  (123)555-0101  123 1st Street  Seattle   \n",
       "1     Owner  (123)555-0100  (123)555-0101  123 2nd Street   Boston   \n",
       "\n",
       "  state_province zip_postal_code country_region  \n",
       "0             WA           99999            USA  \n",
       "1             MA           99999            USA  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"DESCRIBE EXTENDED {dest_database}.dim_customers;\").show()\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.dim_customers LIMIT 2\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e3947d-c807-4bdb-8552-bb6ec0389574",
   "metadata": {},
   "source": [
    "#### 2.4. Populate the <span style=\"color:darkred\">Suppliers Dimension</span>\n",
    "##### 2.3.1. Fetch Data from the New MongoDB <span style=\"color:darkred\">Suppliers</span> Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2c122223-74ee-4dd2-b506-8e6fcd85b028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>first_name</th>\n",
       "      <th>id</th>\n",
       "      <th>job_title</th>\n",
       "      <th>last_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Supplier A</td>\n",
       "      <td>Elizabeth A.</td>\n",
       "      <td>1</td>\n",
       "      <td>Sales Manager</td>\n",
       "      <td>Andersen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Supplier B</td>\n",
       "      <td>Cornelia</td>\n",
       "      <td>2</td>\n",
       "      <td>Sales Manager</td>\n",
       "      <td>Weiler</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      company    first_name  id      job_title last_name\n",
       "0  Supplier A  Elizabeth A.   1  Sales Manager  Andersen\n",
       "1  Supplier B      Cornelia   2  Sales Manager    Weiler"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mongodb_args[\"collection\"] = \"suppliers\"\n",
    "\n",
    "df_dim_suppliers = get_mongodb_dataframe(spark, **mongodb_args)\n",
    "df_dim_suppliers.toPandas().head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcc6f32-aecc-475d-8b37-f775b89f9356",
   "metadata": {},
   "source": [
    "##### 2.3.2. Make Necessary Transformations to the New Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fb757e10-4c6f-4210-b9ce-29894a1d30dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>supplier_key</th>\n",
       "      <th>supplier_id</th>\n",
       "      <th>company</th>\n",
       "      <th>job_title</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Supplier A</td>\n",
       "      <td>Sales Manager</td>\n",
       "      <td>Elizabeth A.</td>\n",
       "      <td>Andersen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Supplier B</td>\n",
       "      <td>Sales Manager</td>\n",
       "      <td>Cornelia</td>\n",
       "      <td>Weiler</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   supplier_key  supplier_id     company      job_title    first_name  \\\n",
       "0             1            1  Supplier A  Sales Manager  Elizabeth A.   \n",
       "1             2            2  Supplier B  Sales Manager      Cornelia   \n",
       "\n",
       "  last_name  \n",
       "0  Andersen  \n",
       "1    Weiler  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------\n",
    "# Rename the 'id' column to 'supplier_id' ------------------------------------------\n",
    "# ----------------------------------------------------------------------------------\n",
    "df_dim_suppliers = df_dim_suppliers.withColumnRenamed(\"id\", \"supplier_id\")\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Add Primary Key column using SQL Windowing function: ROW_NUMBER() \n",
    "# ----------------------------------------------------------------------------------\n",
    "df_dim_suppliers.createOrReplaceTempView(\"suppliers\")\n",
    "sql_suppliers = f\"\"\"\n",
    "    SELECT *, ROW_NUMBER() OVER (ORDER BY supplier_id) AS supplier_key\n",
    "    FROM suppliers;\n",
    "\"\"\"\n",
    "df_dim_suppliers = spark.sql(sql_suppliers)\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Reorder Columns and display the first two rows in a Pandas dataframe\n",
    "# ----------------------------------------------------------------------------------\n",
    "ordered_columns = ['supplier_key', 'supplier_id','company','job_title','first_name','last_name']\n",
    "df_dim_suppliers = df_dim_suppliers[ordered_columns]\n",
    "df_dim_suppliers.toPandas().head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47e1dde-546d-40d9-9c2d-dd56ea7aad03",
   "metadata": {},
   "source": [
    "##### 2.3.3. Save as the <span style=\"color:darkred\">dim_suppliers</span> table in the Data lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "242e4322-4e8a-4279-92b8-0eaa18ee871e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_suppliers.write.saveAsTable(f\"{dest_database}.dim_suppliers\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b525fa75-7fca-445f-a586-5d8acf995f6e",
   "metadata": {},
   "source": [
    "##### 2.3.4. Unit Test: Describe and Preview Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3f390f44-e113-49f3-8ac7-8c864bed2ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|        supplier_key|                 int|   NULL|\n",
      "|         supplier_id|                 int|   NULL|\n",
      "|             company|              string|   NULL|\n",
      "|           job_title|              string|   NULL|\n",
      "|          first_name|              string|   NULL|\n",
      "|           last_name|              string|   NULL|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|             Catalog|       spark_catalog|       |\n",
      "|            Database|       northwind_dlh|       |\n",
      "|               Table|       dim_suppliers|       |\n",
      "|        Created Time|Mon Apr 21 17:26:...|       |\n",
      "|         Last Access|             UNKNOWN|       |\n",
      "|          Created By|         Spark 3.5.5|       |\n",
      "|                Type|             MANAGED|       |\n",
      "|            Provider|             parquet|       |\n",
      "|            Location|file:/Users/sonik...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>supplier_key</th>\n",
       "      <th>supplier_id</th>\n",
       "      <th>company</th>\n",
       "      <th>job_title</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Supplier A</td>\n",
       "      <td>Sales Manager</td>\n",
       "      <td>Elizabeth A.</td>\n",
       "      <td>Andersen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Supplier B</td>\n",
       "      <td>Sales Manager</td>\n",
       "      <td>Cornelia</td>\n",
       "      <td>Weiler</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   supplier_key  supplier_id     company      job_title    first_name  \\\n",
       "0             1            1  Supplier A  Sales Manager  Elizabeth A.   \n",
       "1             2            2  Supplier B  Sales Manager      Cornelia   \n",
       "\n",
       "  last_name  \n",
       "0  Andersen  \n",
       "1    Weiler  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"DESCRIBE EXTENDED {dest_database}.dim_suppliers;\").show()\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.dim_suppliers LIMIT 2\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11cf43d-c4be-446a-bdf9-73400fab4561",
   "metadata": {},
   "source": [
    "#### 2.4. Populate the <span style=\"color:darkred\">Invoices Dimension</span>\n",
    "##### 2.4.1. Fetch Data from the New MongoDB <span style=\"color:darkred\">Invoices</span> Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8c2f01d2-1a8e-445a-9913-549b06c783ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amount_due</th>\n",
       "      <th>id</th>\n",
       "      <th>invoice_date</th>\n",
       "      <th>order_id</th>\n",
       "      <th>shipping</th>\n",
       "      <th>tax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2006-03-22 16:08:59</td>\n",
       "      <td>31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>2006-03-22 16:10:27</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   amount_due  id         invoice_date  order_id  shipping  tax\n",
       "0         0.0   5  2006-03-22 16:08:59        31       0.0  0.0\n",
       "1         0.0   6  2006-03-22 16:10:27        32       0.0  0.0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mongodb_args[\"collection\"] = \"invoices\"\n",
    "\n",
    "df_dim_invoices = get_mongodb_dataframe(spark, **mongodb_args)\n",
    "df_dim_invoices.toPandas().head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe339fa3-907d-4e5a-9f72-3cd71e179a84",
   "metadata": {},
   "source": [
    "##### 2.4.2. Make Necessary Transformations to the New Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8e7dc00b-1241-4463-9d07-35066896cba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>invoice_key</th>\n",
       "      <th>invoice_id</th>\n",
       "      <th>order_id</th>\n",
       "      <th>invoice_date</th>\n",
       "      <th>amount_due</th>\n",
       "      <th>shipping</th>\n",
       "      <th>tax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>31</td>\n",
       "      <td>2006-03-22 16:08:59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>32</td>\n",
       "      <td>2006-03-22 16:10:27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   invoice_key  invoice_id  order_id         invoice_date  amount_due  \\\n",
       "0            1           5        31  2006-03-22 16:08:59         0.0   \n",
       "1            2           6        32  2006-03-22 16:10:27         0.0   \n",
       "\n",
       "   shipping  tax  \n",
       "0       0.0  0.0  \n",
       "1       0.0  0.0  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------\n",
    "# Rename the 'id' column to 'invoice_id' ------------------------------------------\n",
    "# ----------------------------------------------------------------------------------\n",
    "df_dim_invoices = df_dim_invoices.withColumnRenamed(\"id\", \"invoice_id\")\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Add Primary Key column using SQL Windowing function: ROW_NUMBER() \n",
    "# ----------------------------------------------------------------------------------\n",
    "df_dim_invoices.createOrReplaceTempView(\"invoices\")\n",
    "sql_invoices = f\"\"\"\n",
    "    SELECT *, ROW_NUMBER() OVER (ORDER BY invoice_id) AS invoice_key\n",
    "    FROM invoices;\n",
    "\"\"\"\n",
    "df_dim_invoices = spark.sql(sql_invoices)\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Reorder Columns and display the first two rows in a Pandas dataframe\n",
    "# ----------------------------------------------------------------------------------\n",
    "ordered_columns = ['invoice_key', 'invoice_id','order_id', 'invoice_date','amount_due', 'shipping', 'tax']\n",
    "df_dim_invoices = df_dim_invoices[ordered_columns]\n",
    "df_dim_invoices.toPandas().head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c3fc36-487a-4342-b47f-c2cbc04f27b4",
   "metadata": {},
   "source": [
    "##### 2.4.3. Save as the <span style=\"color:darkred\">dim_invoices</span> table in the Data lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "76518f9d-133a-4c8a-969b-6ed014fd028f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_invoices.write.saveAsTable(f\"{dest_database}.dim_invoices\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e9d072-764d-478e-80d7-f5b84f6618b3",
   "metadata": {},
   "source": [
    "##### 2.4.4. Unit Test: Describe and Preview Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8b046075-3b8f-45fb-a5ce-235862429603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|         invoice_key|                 int|   NULL|\n",
      "|          invoice_id|                 int|   NULL|\n",
      "|            order_id|                 int|   NULL|\n",
      "|        invoice_date|              string|   NULL|\n",
      "|          amount_due|              double|   NULL|\n",
      "|            shipping|              double|   NULL|\n",
      "|                 tax|              double|   NULL|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|             Catalog|       spark_catalog|       |\n",
      "|            Database|       northwind_dlh|       |\n",
      "|               Table|        dim_invoices|       |\n",
      "|        Created Time|Mon Apr 21 17:26:...|       |\n",
      "|         Last Access|             UNKNOWN|       |\n",
      "|          Created By|         Spark 3.5.5|       |\n",
      "|                Type|             MANAGED|       |\n",
      "|            Provider|             parquet|       |\n",
      "|            Location|file:/Users/sonik...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>invoice_key</th>\n",
       "      <th>invoice_id</th>\n",
       "      <th>order_id</th>\n",
       "      <th>invoice_date</th>\n",
       "      <th>amount_due</th>\n",
       "      <th>shipping</th>\n",
       "      <th>tax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>31</td>\n",
       "      <td>2006-03-22 16:08:59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>32</td>\n",
       "      <td>2006-03-22 16:10:27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   invoice_key  invoice_id  order_id         invoice_date  amount_due  \\\n",
       "0            1           5        31  2006-03-22 16:08:59         0.0   \n",
       "1            2           6        32  2006-03-22 16:10:27         0.0   \n",
       "\n",
       "   shipping  tax  \n",
       "0       0.0  0.0  \n",
       "1       0.0  0.0  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"DESCRIBE EXTENDED {dest_database}.dim_invoices;\").show()\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.dim_invoices LIMIT 2\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8947777-e4a6-461f-9490-301ccaf4b06a",
   "metadata": {},
   "source": [
    "### 3.0. Fetch Reference Data from a MySQL Database\n",
    "#### 3.1. Populate the <span style=\"color:darkred\">Date Dimension</span>\n",
    "##### 3.1.1 Fetch data from the <span style=\"color:darkred\">dim_date</span> table in MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0f4af01d-9f24-4287-87cd-07fc31502958",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_dim_date = f\"SELECT * FROM {mysql_args['db_name']}.dim_date\"\n",
    "df_dim_date = get_mysql_dataframe(spark, sql_dim_date, **mysql_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23be56fc-b6e3-42e1-801d-3474957aee4a",
   "metadata": {},
   "source": [
    "##### 3.1.2. Save as the <span style=\"color:darkred\">dim_date</span> table in the Data Lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "868fc437-5f13-447e-8def-1fc663eb05a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_date.write.saveAsTable(f\"{dest_database}.dim_date\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f64095-6b91-43b4-b96f-20d4fafa9a35",
   "metadata": {},
   "source": [
    "##### 3.1.3. Unit Test: Describe and Preview Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a08f8ad8-b44d-4a82-bd4e-d94757964c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------+\n",
      "|            col_name|data_type|comment|\n",
      "+--------------------+---------+-------+\n",
      "|            date_key|      int|   NULL|\n",
      "|           full_date|     date|   NULL|\n",
      "|           date_name| char(11)|   NULL|\n",
      "|        date_name_us| char(11)|   NULL|\n",
      "|        date_name_eu| char(11)|   NULL|\n",
      "|         day_of_week|  tinyint|   NULL|\n",
      "|    day_name_of_week| char(10)|   NULL|\n",
      "|        day_of_month|  tinyint|   NULL|\n",
      "|         day_of_year|      int|   NULL|\n",
      "|     weekday_weekend| char(10)|   NULL|\n",
      "|        week_of_year|  tinyint|   NULL|\n",
      "|          month_name| char(10)|   NULL|\n",
      "|       month_of_year|  tinyint|   NULL|\n",
      "|is_last_day_of_month|  char(1)|   NULL|\n",
      "|    calendar_quarter|  tinyint|   NULL|\n",
      "|       calendar_year|      int|   NULL|\n",
      "| calendar_year_month| char(10)|   NULL|\n",
      "|   calendar_year_qtr| char(10)|   NULL|\n",
      "|fiscal_month_of_year|  tinyint|   NULL|\n",
      "|      fiscal_quarter|  tinyint|   NULL|\n",
      "+--------------------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_key</th>\n",
       "      <th>full_date</th>\n",
       "      <th>date_name</th>\n",
       "      <th>date_name_us</th>\n",
       "      <th>date_name_eu</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>day_name_of_week</th>\n",
       "      <th>day_of_month</th>\n",
       "      <th>day_of_year</th>\n",
       "      <th>weekday_weekend</th>\n",
       "      <th>...</th>\n",
       "      <th>is_last_day_of_month</th>\n",
       "      <th>calendar_quarter</th>\n",
       "      <th>calendar_year</th>\n",
       "      <th>calendar_year_month</th>\n",
       "      <th>calendar_year_qtr</th>\n",
       "      <th>fiscal_month_of_year</th>\n",
       "      <th>fiscal_quarter</th>\n",
       "      <th>fiscal_year</th>\n",
       "      <th>fiscal_year_month</th>\n",
       "      <th>fiscal_year_qtr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20000101</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>2000/01/01</td>\n",
       "      <td>01/01/2000</td>\n",
       "      <td>01/01/2000</td>\n",
       "      <td>7</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Weekend</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000-01</td>\n",
       "      <td>2000Q1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000-07</td>\n",
       "      <td>2000Q3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20000102</td>\n",
       "      <td>2000-01-02</td>\n",
       "      <td>2000/01/02</td>\n",
       "      <td>01/02/2000</td>\n",
       "      <td>02/01/2000</td>\n",
       "      <td>1</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Weekend</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000-01</td>\n",
       "      <td>2000Q1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000-07</td>\n",
       "      <td>2000Q3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows  23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   date_key   full_date    date_name date_name_us date_name_eu  day_of_week  \\\n",
       "0  20000101  2000-01-01  2000/01/01   01/01/2000   01/01/2000             7   \n",
       "1  20000102  2000-01-02  2000/01/02   01/02/2000   02/01/2000             1   \n",
       "\n",
       "  day_name_of_week  day_of_month  day_of_year weekday_weekend  ...  \\\n",
       "0       Saturday               1            1      Weekend     ...   \n",
       "1       Sunday                 2            2      Weekend     ...   \n",
       "\n",
       "   is_last_day_of_month calendar_quarter  calendar_year calendar_year_month  \\\n",
       "0                     N                1           2000          2000-01      \n",
       "1                     N                1           2000          2000-01      \n",
       "\n",
       "   calendar_year_qtr  fiscal_month_of_year fiscal_quarter fiscal_year  \\\n",
       "0         2000Q1                         7              3        2000   \n",
       "1         2000Q1                         7              3        2000   \n",
       "\n",
       "   fiscal_year_month  fiscal_year_qtr  \n",
       "0         2000-07          2000Q3      \n",
       "1         2000-07          2000Q3      \n",
       "\n",
       "[2 rows x 23 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"DESCRIBE EXTENDED {dest_database}.dim_date;\").show()\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.dim_date LIMIT 2\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df0a18f-6a72-4b02-b4a7-f41080d0ca4d",
   "metadata": {},
   "source": [
    "#### 3.2. Populate the <span style=\"color:darkred\">Product Dimension</span>\n",
    "##### 3.2.1. Fetch data from the <span style=\"color:darkred\">Products</span> table in MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ff551005-e91d-4dd4-9b2a-36326b0d73ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(supplier_ids='4', id=1, product_code='NWTB-1', product_name='Northwind Traders Chai', description=None, standard_cost=Decimal('13.5000'), list_price=Decimal('18.0000'), reorder_level=10, target_level=40, quantity_per_unit='10 boxes x 20 bags', discontinued=False, minimum_reorder_quantity=10, category='Beverages', attachments=bytearray(b''), product_key=1)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------\n",
    "# Add Primary Key column using the SQL Windowing function: ROW_NUMBER() \n",
    "# ----------------------------------------------------------------------------------\n",
    "sql_products = f\"SELECT * FROM {mysql_args['db_name']}.products\"\n",
    "df_dim_products = get_mysql_dataframe(spark, sql_products, **mysql_args)\n",
    "\n",
    "df_dim_products.createOrReplaceTempView(\"products\")\n",
    "sql_products = f\"\"\"\n",
    "    SELECT *, ROW_NUMBER() OVER (ORDER BY id) AS product_key\n",
    "    FROM products;\n",
    "\"\"\"\n",
    "df_dim_products = spark.sql(sql_products)\n",
    "df_dim_products.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b715baa-4e46-40dd-9cd1-eeafbaefa977",
   "metadata": {},
   "source": [
    "##### 3.2.2. Perform any Necessary Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "26d6d060-81e9-4929-b41a-a8444ec62405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_key</th>\n",
       "      <th>product_id</th>\n",
       "      <th>supplier_ids</th>\n",
       "      <th>product_code</th>\n",
       "      <th>product_name</th>\n",
       "      <th>standard_cost</th>\n",
       "      <th>list_price</th>\n",
       "      <th>reorder_level</th>\n",
       "      <th>target_level</th>\n",
       "      <th>quantity_per_unit</th>\n",
       "      <th>discontinued</th>\n",
       "      <th>minimum_reorder_quantity</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NWTB-1</td>\n",
       "      <td>Northwind Traders Chai</td>\n",
       "      <td>13.5000</td>\n",
       "      <td>18.0000</td>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "      <td>10 boxes x 20 bags</td>\n",
       "      <td>False</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Beverages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>NWTCO-3</td>\n",
       "      <td>Northwind Traders Syrup</td>\n",
       "      <td>7.5000</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>25</td>\n",
       "      <td>100</td>\n",
       "      <td>12 - 550 ml bottles</td>\n",
       "      <td>False</td>\n",
       "      <td>25.0</td>\n",
       "      <td>Condiments</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_key  product_id supplier_ids product_code             product_name  \\\n",
       "0            1           1            4       NWTB-1   Northwind Traders Chai   \n",
       "1            2           3           10      NWTCO-3  Northwind Traders Syrup   \n",
       "\n",
       "  standard_cost list_price  reorder_level  target_level    quantity_per_unit  \\\n",
       "0       13.5000    18.0000             10            40   10 boxes x 20 bags   \n",
       "1        7.5000    10.0000             25           100  12 - 550 ml bottles   \n",
       "\n",
       "   discontinued  minimum_reorder_quantity    category  \n",
       "0         False                      10.0   Beverages  \n",
       "1         False                      25.0  Condiments  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------\n",
    "# Rename the 'id' column to 'product_id' \n",
    "# ----------------------------------------------------------------------------------\n",
    "# Using the monotonically_increasing_id() function has some limitations: starts with zero (0), and is not sequential.\n",
    "    # df_dim_products = df_dim_products.withColumn(\"product_key\", monotonically_increasing_id())\n",
    "df_dim_products = df_dim_products.withColumnRenamed(\"id\", \"product_id\")\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Drop unwanted columns (description and attachments)\n",
    "# ----------------------------------------------------------------------------------\n",
    "df_dim_products.drop(\"description\",\"attachments\")\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Reorder Columns and display the first two rows in a Pandas dataframe\n",
    "# ----------------------------------------------------------------------------------\n",
    "ordered_columns = ['product_key','product_id','supplier_ids','product_code','product_name','standard_cost','list_price','reorder_level','target_level','quantity_per_unit','discontinued','minimum_reorder_quantity','category']\n",
    "df_dim_products = df_dim_products[ordered_columns]\n",
    "df_dim_products.toPandas().head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e24d3a-7dfd-45d6-ae1e-a5023ed4e420",
   "metadata": {},
   "source": [
    "##### 3.2.3. Save as the <span style=\"color:darkred\">dim_products</span> table in the Data Lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b76683fe-db95-4ab7-934e-e3d28bc4e717",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_products.write.saveAsTable(f\"{dest_database}.dim_products\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6942bdac-df24-4e80-bd1b-e1dc276d2c34",
   "metadata": {},
   "source": [
    "##### 3.2.4. Unit Test: Describe and Preview Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7a723db4-148f-4a75-a40c-dac99aab3559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|         product_key|                 int|   NULL|\n",
      "|          product_id|                 int|   NULL|\n",
      "|        supplier_ids|              string|   NULL|\n",
      "|        product_code|         varchar(25)|   NULL|\n",
      "|        product_name|         varchar(50)|   NULL|\n",
      "|       standard_cost|       decimal(19,4)|   NULL|\n",
      "|          list_price|       decimal(19,4)|   NULL|\n",
      "|       reorder_level|                 int|   NULL|\n",
      "|        target_level|                 int|   NULL|\n",
      "|   quantity_per_unit|         varchar(50)|   NULL|\n",
      "|        discontinued|             boolean|   NULL|\n",
      "|minimum_reorder_q...|                 int|   NULL|\n",
      "|            category|         varchar(50)|   NULL|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|             Catalog|       spark_catalog|       |\n",
      "|            Database|       northwind_dlh|       |\n",
      "|               Table|        dim_products|       |\n",
      "|        Created Time|Mon Apr 21 17:26:...|       |\n",
      "|         Last Access|             UNKNOWN|       |\n",
      "+--------------------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_key</th>\n",
       "      <th>product_id</th>\n",
       "      <th>supplier_ids</th>\n",
       "      <th>product_code</th>\n",
       "      <th>product_name</th>\n",
       "      <th>standard_cost</th>\n",
       "      <th>list_price</th>\n",
       "      <th>reorder_level</th>\n",
       "      <th>target_level</th>\n",
       "      <th>quantity_per_unit</th>\n",
       "      <th>discontinued</th>\n",
       "      <th>minimum_reorder_quantity</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NWTB-1</td>\n",
       "      <td>Northwind Traders Chai</td>\n",
       "      <td>13.5000</td>\n",
       "      <td>18.0000</td>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "      <td>10 boxes x 20 bags</td>\n",
       "      <td>False</td>\n",
       "      <td>10</td>\n",
       "      <td>Beverages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>NWTCO-3</td>\n",
       "      <td>Northwind Traders Syrup</td>\n",
       "      <td>7.5000</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>25</td>\n",
       "      <td>100</td>\n",
       "      <td>12 - 550 ml bottles</td>\n",
       "      <td>False</td>\n",
       "      <td>25</td>\n",
       "      <td>Condiments</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_key  product_id supplier_ids product_code             product_name  \\\n",
       "0            1           1            4       NWTB-1   Northwind Traders Chai   \n",
       "1            2           3           10      NWTCO-3  Northwind Traders Syrup   \n",
       "\n",
       "  standard_cost list_price  reorder_level  target_level    quantity_per_unit  \\\n",
       "0       13.5000    18.0000             10            40   10 boxes x 20 bags   \n",
       "1        7.5000    10.0000             25           100  12 - 550 ml bottles   \n",
       "\n",
       "   discontinued  minimum_reorder_quantity    category  \n",
       "0         False                        10   Beverages  \n",
       "1         False                        25  Condiments  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"DESCRIBE EXTENDED {dest_database}.dim_products;\").show()\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.dim_products LIMIT 2\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375bf727-5f3e-4589-a4da-d57446a376f3",
   "metadata": {},
   "source": [
    "### 4.0. Verify Dimension Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "29795432-f447-4c0b-a5a1-cc7d4b4fb008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>namespace</th>\n",
       "      <th>tableName</th>\n",
       "      <th>isTemporary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>northwind_dlh</td>\n",
       "      <td>dim_customers</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>northwind_dlh</td>\n",
       "      <td>dim_date</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>northwind_dlh</td>\n",
       "      <td>dim_employees</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>northwind_dlh</td>\n",
       "      <td>dim_invoices</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>northwind_dlh</td>\n",
       "      <td>dim_products</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>northwind_dlh</td>\n",
       "      <td>dim_shippers</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>northwind_dlh</td>\n",
       "      <td>dim_suppliers</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "      <td>customers</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td></td>\n",
       "      <td>employees</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td></td>\n",
       "      <td>invoices</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td></td>\n",
       "      <td>products</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td></td>\n",
       "      <td>shippers</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td></td>\n",
       "      <td>suppliers</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        namespace      tableName  isTemporary\n",
       "0   northwind_dlh  dim_customers        False\n",
       "1   northwind_dlh       dim_date        False\n",
       "2   northwind_dlh  dim_employees        False\n",
       "3   northwind_dlh   dim_invoices        False\n",
       "4   northwind_dlh   dim_products        False\n",
       "5   northwind_dlh   dim_shippers        False\n",
       "6   northwind_dlh  dim_suppliers        False\n",
       "7                      customers         True\n",
       "8                      employees         True\n",
       "9                       invoices         True\n",
       "10                      products         True\n",
       "11                      shippers         True\n",
       "12                     suppliers         True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"USE {dest_database};\")\n",
    "spark.sql(\"SHOW TABLES\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f158f3-15d4-4b77-8080-c3d1bc68986a",
   "metadata": {},
   "source": [
    "## Section III: Integrate Reference Data with Real-Time Data\n",
    "### 6.0. Use PySpark Structured Streaming to Process (Hot Path) <span style=\"color:darkred\">Orders</span> Fact Data  \n",
    "#### 6.1. Verify the location of the source data files on the file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2b506c14-a897-4ac8-a01e-590ddc2e49b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>size</th>\n",
       "      <th>modification_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>northwind_orders_01.json</td>\n",
       "      <td>9609</td>\n",
       "      <td>2025-04-01 16:28:26.273692846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>northwind_orders_02.json</td>\n",
       "      <td>9103</td>\n",
       "      <td>2025-04-01 16:28:26.273750544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>northwind_orders_03.json</td>\n",
       "      <td>9008</td>\n",
       "      <td>2025-04-01 16:28:26.273926020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       name  size             modification_time\n",
       "0  northwind_orders_01.json  9609 2025-04-01 16:28:26.273692846\n",
       "1  northwind_orders_02.json  9103 2025-04-01 16:28:26.273750544\n",
       "2  northwind_orders_03.json  9008 2025-04-01 16:28:26.273926020"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_file_info(orders_stream_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725f2bda-d78e-4731-9d4e-10d75d3607ba",
   "metadata": {},
   "source": [
    "#### 6.2. Create the Bronze Layer: Stage <span style=\"color:darkred\">Orders Fact table</span> Data\n",
    "##### 6.2.1. Read \"Raw\" JSON file data into a Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fe375157-dfad-4442-8563-0a05cacecb55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orders_bronze = (\n",
    "    spark.readStream \\\n",
    "    .option(\"schemaLocation\", orders_output_bronze) \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .option(\"multiLine\", \"true\") \\\n",
    "    .json(orders_stream_dir)\n",
    ")\n",
    "\n",
    "df_orders_bronze.isStreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e94035a-3bc4-4ba2-abf5-63ca89b4e0f7",
   "metadata": {},
   "source": [
    "##### 6.2.2. Write the Streaming Data to a Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3cd60124-791d-4df3-99eb-a5943c5b3f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_checkpoint_bronze = os.path.join(orders_output_bronze, '_checkpoint')\n",
    "\n",
    "orders_bronze_query = (\n",
    "    df_orders_bronze\n",
    "    # Add Current Timestamp and Input Filename columns for Traceability\n",
    "    .withColumn(\"receipt_time\", current_timestamp())\n",
    "    .withColumn(\"source_file\", input_file_name())\n",
    "    \n",
    "    .writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .queryName(\"orders_bronze\")\n",
    "    .trigger(availableNow = True) \\\n",
    "    .option(\"checkpointLocation\", orders_checkpoint_bronze) \\\n",
    "    .option(\"compression\", \"snappy\") \\\n",
    "    .start(orders_output_bronze)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45b461d-1d9c-4233-aae9-cae37539c0ba",
   "metadata": {},
   "source": [
    "##### 6.2.3. Unit Test: Implement Query Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "299479ba-fb32-4260-8585-14f9c47b1b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: 1799c556-3284-4956-8a04-3d76c0c86482\n",
      "Query Name: orders_bronze\n",
      "Query Status: {'message': 'Initializing sources', 'isDataAvailable': False, 'isTriggerActive': False}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Query ID: {orders_bronze_query.id}\")\n",
    "print(f\"Query Name: {orders_bronze_query.name}\")\n",
    "print(f\"Query Status: {orders_bronze_query.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bc62d72c-f857-43c0-8c37-716ef3501064",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_bronze_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1baf025-d6fd-4b69-89d2-dd89c3205496",
   "metadata": {},
   "source": [
    "#### 6.3. Create the Silver Layer: Integrate \"Cold-path\" Data & Make Transformations\n",
    "##### 6.3.1. Prepare Role-Playing Dimension Primary and Business Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "28b1f33c-0ebd-4156-9f14-e8588cab2c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_order_date = df_dim_date.select(col(\"date_key\").alias(\"order_date_key\"), col(\"full_date\").alias(\"order_full_date\"))\n",
    "df_dim_paid_date = df_dim_date.select(col(\"date_key\").alias(\"paid_date_key\"), col(\"full_date\").alias(\"paid_full_date\"))\n",
    "df_dim_shipped_date = df_dim_date.select(col(\"date_key\").alias(\"shipped_date_key\"), col(\"full_date\").alias(\"shipped_full_date\"))\n",
    "df_dim_shippers = df_dim_shippers.withColumnRenamed(\"shipper_id\", \"shipper_no\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7b0437-66a7-4386-9084-4594faab6df0",
   "metadata": {},
   "source": [
    "##### 6.3.2. Define Silver Query to Join Streaming with Batch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1f8ea470-0cc3-4530-99de-8cf5beae64cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_silver = spark.readStream.format(\"parquet\").load(orders_output_bronze) \\\n",
    "    .join(df_dim_customers, \"customer_id\") \\\n",
    "    .join(df_dim_employees, \"employee_id\") \\\n",
    "    .join(df_dim_products, \"product_id\") \\\n",
    "    .join(df_dim_shippers, df_dim_shippers.shipper_no == col(\"shipper_id\").cast(IntegerType()), \"left_outer\") \\\n",
    "    .join(df_dim_order_date, df_dim_order_date.order_full_date.cast(DateType()) == col(\"order_date\").cast(DateType()), \"inner\") \\\n",
    "    .join(df_dim_shipped_date, df_dim_shipped_date.shipped_full_date.cast(DateType()) == col(\"shipped_date\").cast(DateType()), \"left_outer\") \\\n",
    "    .join(df_dim_paid_date, df_dim_paid_date.paid_full_date.cast(DateType()) == col(\"paid_date\").cast(DateType()), \"left_outer\") \\\n",
    "    .select(col(\"order_id\").cast(LongType()), \\\n",
    "            col(\"order_detail_id\").cast(LongType()), \\\n",
    "            df_dim_customers.customer_key.cast(LongType()), \\\n",
    "            df_dim_employees.employee_key.cast(LongType()), \\\n",
    "            df_dim_products.product_key.cast(LongType()), \\\n",
    "            df_dim_shippers.shipper_key.cast(IntegerType()), \\\n",
    "            df_dim_order_date.order_date_key.cast(LongType()), \\\n",
    "            df_dim_paid_date.paid_date_key.cast(LongType()), \\\n",
    "            df_dim_shipped_date.shipped_date_key.cast(LongType()), \\\n",
    "            col(\"quantity\"), \\\n",
    "            col(\"unit_price\"), \\\n",
    "            col(\"discount\"), \\\n",
    "            col(\"shipping_fee\"), \\\n",
    "            col(\"taxes\"), \\\n",
    "            col(\"tax_rate\"), \\\n",
    "            col(\"payment_type\"), \\\n",
    "            col(\"order_status\"), \\\n",
    "            col(\"order_details_status\") \\\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cce36782-37f7-4a7a-baf8-bc6c9d7e647d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orders_silver.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "adcb25db-964b-41ed-bfc8-9ca633c219b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- order_detail_id: long (nullable = true)\n",
      " |-- customer_key: long (nullable = false)\n",
      " |-- employee_key: long (nullable = false)\n",
      " |-- product_key: long (nullable = false)\n",
      " |-- shipper_key: integer (nullable = true)\n",
      " |-- order_date_key: long (nullable = true)\n",
      " |-- paid_date_key: long (nullable = true)\n",
      " |-- shipped_date_key: long (nullable = true)\n",
      " |-- quantity: double (nullable = true)\n",
      " |-- unit_price: double (nullable = true)\n",
      " |-- discount: long (nullable = true)\n",
      " |-- shipping_fee: double (nullable = true)\n",
      " |-- taxes: double (nullable = true)\n",
      " |-- tax_rate: long (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_details_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_orders_silver.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6c9fd4-c595-4c38-adb3-c5a3a1f13f53",
   "metadata": {},
   "source": [
    "##### 6.3.3. Write the Transformed Streaming data to the Data Lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "145b60d0-0985-4641-822a-7e7995ce0785",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_checkpoint_silver = os.path.join(orders_output_silver, '_checkpoint')\n",
    "\n",
    "orders_silver_query = (\n",
    "    df_orders_silver.writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .queryName(\"orders_silver\")\n",
    "    .trigger(availableNow = True) \\\n",
    "    .option(\"checkpointLocation\", orders_checkpoint_silver) \\\n",
    "    .option(\"compression\", \"snappy\") \\\n",
    "    .start(orders_output_silver)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a6c445-16dd-4752-9e46-6ba7e9cef121",
   "metadata": {},
   "source": [
    "##### 6.3.4. Unit Test: Implement Query Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ea1e7be2-fea6-4578-9237-344983c52111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: 0771b357-9e10-404f-9b97-fafb9e5e5546\n",
      "Query Name: orders_silver\n",
      "Query Status: {'message': 'Initializing sources', 'isDataAvailable': False, 'isTriggerActive': False}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Query ID: {orders_silver_query.id}\")\n",
    "print(f\"Query Name: {orders_silver_query.name}\")\n",
    "print(f\"Query Status: {orders_silver_query.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1493145c-19b3-4446-84e1-b9a37ecf2010",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_silver_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb44a5a-b470-4928-bc73-b5ed9ff8da74",
   "metadata": {},
   "source": [
    "#### 6.4. Create Gold Layer: Perform Aggregations\n",
    "##### 6.4.1. Define a Query to Create a Business Report\n",
    "Create a new Gold table using the PySpark API. The table should include the number of Products sold per Category each Month. The results should include The Month, Product Category and Number of Products sold, sorted by the month number when the orders were placed: e.g., January, February, March."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2b0bb272-a7c1-4f2a-a3d9-5ee5fd6f454a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_by_product_category_gold = spark.readStream.format(\"parquet\").load(orders_output_silver) \\\n",
    ".join(df_dim_products, \"product_key\") \\\n",
    ".join(df_dim_date, df_dim_date.date_key.cast(IntegerType()) == col(\"order_date_key\").cast(IntegerType())) \\\n",
    ".groupBy(\"month_of_year\", \"category\", \"month_name\") \\\n",
    ".agg(count(\"product_key\").alias(\"product_count\")) \\\n",
    ".orderBy(asc(\"month_of_year\"), desc(\"product_count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "36930416-660d-4814-a66c-a08d732ba921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- month_of_year: byte (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- month_name: string (nullable = true)\n",
      " |-- product_count: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_orders_by_product_category_gold.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb8abc8-c889-4ed2-8d37-f15bb1c27ffb",
   "metadata": {},
   "source": [
    "##### 6.4.2. Write the Streaming data to a Parquet File in \"Complete\" mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0619456b-2217-4a27-b9cf-5d764610a6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_gold_query = (\n",
    "    df_orders_by_product_category_gold.writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .queryName(\"fact_orders_by_product_category\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e88c1d0b-8341-4c5a-9970-c7e2f444feca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stream has processed 1 batchs\n"
     ]
    }
   ],
   "source": [
    "wait_until_stream_is_ready(orders_gold_query, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b627bd3d-d17b-4ab2-b62a-1b988d10a7c6",
   "metadata": {},
   "source": [
    "##### 6.4.3. Query the Gold Data from Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c1d4cf7a-de59-4596-a8af-0e84790d7b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- month_of_year: byte (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- month_name: string (nullable = true)\n",
      " |-- product_count: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fact_orders_by_product_category = spark.sql(\"SELECT * FROM fact_orders_by_product_category\")\n",
    "df_fact_orders_by_product_category.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7c34a4-74ab-45cb-b096-fdd860a00012",
   "metadata": {},
   "source": [
    "##### 6.4.4 Create the Final Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "83ba5a02-ea03-4f9e-99d8-f50327779467",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact_orders_by_product_category_gold_final = df_fact_orders_by_product_category \\\n",
    ".select(col(\"month_name\").alias(\"Month\"), \\\n",
    "        col(\"category\").alias(\"Product Category\"), \\\n",
    "        col(\"product_count\").alias(\"Product Count\")) \\\n",
    ".orderBy(asc(\"month_of_year\"), desc(\"Product Count\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c010c836-3593-4ff5-afb1-f5b29798f3dd",
   "metadata": {},
   "source": [
    "##### 6.4.5. Load the Final Results into a New Table and Display the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3f7ba782-5dca-4030-b386-04e8f7c5e249",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month</th>\n",
       "      <th>Product Category</th>\n",
       "      <th>Product Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>March</td>\n",
       "      <td>Sauces</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>March</td>\n",
       "      <td>Dried Fruit &amp; Nuts</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>March</td>\n",
       "      <td>Jams, Preserves</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>March</td>\n",
       "      <td>Candy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>March</td>\n",
       "      <td>Condiments</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>March</td>\n",
       "      <td>Baked Goods &amp; Mixes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>April</td>\n",
       "      <td>Baked Goods &amp; Mixes</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>April</td>\n",
       "      <td>Sauces</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>April</td>\n",
       "      <td>Canned Meat</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>April</td>\n",
       "      <td>Dairy products</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>April</td>\n",
       "      <td>Soups</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>January</td>\n",
       "      <td>Dried Fruit &amp; Nuts</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>January</td>\n",
       "      <td>Beverages</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>January</td>\n",
       "      <td>Baked Goods &amp; Mixes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>June</td>\n",
       "      <td>Jams, Preserves</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>June</td>\n",
       "      <td>Beverages</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>June</td>\n",
       "      <td>Condiments</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>June</td>\n",
       "      <td>Soups</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>June</td>\n",
       "      <td>Canned Fruit &amp; Vegetables</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>February</td>\n",
       "      <td>Candy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>February</td>\n",
       "      <td>Soups</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>February</td>\n",
       "      <td>Baked Goods &amp; Mixes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>April</td>\n",
       "      <td>Condiments</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>April</td>\n",
       "      <td>Oil</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>April</td>\n",
       "      <td>Jams, Preserves</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>April</td>\n",
       "      <td>Candy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>April</td>\n",
       "      <td>Grains</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>June</td>\n",
       "      <td>Dried Fruit &amp; Nuts</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>June</td>\n",
       "      <td>Candy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>May</td>\n",
       "      <td>Dried Fruit &amp; Nuts</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>May</td>\n",
       "      <td>Canned Meat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>May</td>\n",
       "      <td>Sauces</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>April</td>\n",
       "      <td>Beverages</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>April</td>\n",
       "      <td>Pasta</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>March</td>\n",
       "      <td>Beverages</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Month           Product Category  Product Count\n",
       "0   March                          Sauces              1\n",
       "1   March              Dried Fruit & Nuts              1\n",
       "2   March                 Jams, Preserves              1\n",
       "3   March                           Candy              1\n",
       "4   March                      Condiments              1\n",
       "5   March             Baked Goods & Mixes              1\n",
       "6   April             Baked Goods & Mixes              2\n",
       "7   April                          Sauces              2\n",
       "8   April                     Canned Meat              2\n",
       "9   April                  Dairy products              2\n",
       "10  April                           Soups              2\n",
       "11  January            Dried Fruit & Nuts              4\n",
       "12  January                     Beverages              3\n",
       "13  January           Baked Goods & Mixes              1\n",
       "14  June                  Jams, Preserves              1\n",
       "15  June                        Beverages              1\n",
       "16  June                       Condiments              1\n",
       "17  June                            Soups              1\n",
       "18  June        Canned Fruit & Vegetables              1\n",
       "19  February                        Candy              1\n",
       "20  February                        Soups              1\n",
       "21  February          Baked Goods & Mixes              1\n",
       "22  April                      Condiments              1\n",
       "23  April                             Oil              1\n",
       "24  April                 Jams, Preserves              1\n",
       "25  April                           Candy              1\n",
       "26  April                          Grains              1\n",
       "27  June               Dried Fruit & Nuts              2\n",
       "28  June                            Candy              2\n",
       "29  May                Dried Fruit & Nuts              2\n",
       "30  May                       Canned Meat              1\n",
       "31  May                            Sauces              1\n",
       "32  April                       Beverages              3\n",
       "33  April                           Pasta              3\n",
       "34  March                       Beverages              7"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fact_orders_by_product_category_gold_final.write.saveAsTable(f\"{dest_database}.fact_orders_by_product_category\", mode=\"overwrite\")\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.fact_orders_by_product_category\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be545e39-4352-4688-b7c1-92097acd2e88",
   "metadata": {},
   "source": [
    "### 7.0. Use PySpark Structured Streaming to Process (Hot Path) <span style=\"color:darkred\">Inventory Transactions</span> Fact Data\n",
    "#### 7.1. Verify the location of the source data files on the file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "cbb671dd-1607-41d4-8ebd-e1edce43311a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>size</th>\n",
       "      <th>modification_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>northwind_inventory_transactions_01.json</td>\n",
       "      <td>7656</td>\n",
       "      <td>2025-04-01 16:28:26.273453951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>northwind_inventory_transactions_02.json</td>\n",
       "      <td>7590</td>\n",
       "      <td>2025-04-01 16:28:26.273522615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>northwind_inventory_transactions_03.json</td>\n",
       "      <td>7587</td>\n",
       "      <td>2025-04-01 16:28:26.273586988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       name  size  \\\n",
       "0  northwind_inventory_transactions_01.json  7656   \n",
       "1  northwind_inventory_transactions_02.json  7590   \n",
       "2  northwind_inventory_transactions_03.json  7587   \n",
       "\n",
       "              modification_time  \n",
       "0 2025-04-01 16:28:26.273453951  \n",
       "1 2025-04-01 16:28:26.273522615  \n",
       "2 2025-04-01 16:28:26.273586988  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_file_info(inventory_trans_stream_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4029fc9f-6ed7-48f6-a347-0245926a542f",
   "metadata": {},
   "source": [
    "#### 7.2. Create the Bronze Layer: Stage <span style=\"color:darkred\">Inventory Transactions Fact table</span> Data\n",
    "##### 7.2.1. Read \"Raw\" JSON file data into a Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2f047ae2-f893-4dea-884b-f2e5ebdd5253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO: load data from 'inventory_trans_stream_dir'\n",
    "df_inventory_trans_bronze = (\n",
    "    spark.readStream \\\n",
    "    .option(\"schemaLocation\", inventory_trans_output_bronze) \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .option(\"multiLine\", \"true\") \\\n",
    "    .json(inventory_trans_stream_dir)\n",
    ")\n",
    "\n",
    "df_inventory_trans_bronze.isStreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739e693f-7dcb-4e8d-ac6a-dd12b19c32ad",
   "metadata": {},
   "source": [
    "##### 7.2.2. Write the Streaming Data to a Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3598e1e1-3c23-4421-8cd9-da6c93133cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory_trans_checkpoint_bronze = os.path.join(inventory_trans_output_bronze, '_checkpoint')\n",
    "\n",
    "inventory_trans_bronze_query = (\n",
    "    df_inventory_trans_bronze\n",
    "    # TODO: Add Current Timestamp and Input Filename columns for Traceability\n",
    "    .withColumn(\"receipt_time\", current_timestamp())\n",
    "    .withColumn(\"source_file\", input_file_name())\n",
    "    \n",
    "    # TODO: writeStream to 'inventory_trans_output_bronze' in 'append' mode\n",
    "    .writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .queryName(\"inventory_trans_bronze\") \\\n",
    "    .trigger(availableNow = True) \\\n",
    "    .option(\"checkpointLocation\", inventory_trans_checkpoint_bronze) \\\n",
    "    .option(\"compression\", \"snappy\") \\\n",
    "    .start(inventory_trans_output_bronze)  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb998485-8a0f-4ba1-b51c-03836b36f1af",
   "metadata": {},
   "source": [
    "##### 7.2.3. Unit Test: Implement Query Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0b12a2c9-d8fb-4df0-b146-46ce1a1b1eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: d715df28-746c-491b-b07b-20f7477db93d\n",
      "Query Name: inventory_trans_bronze\n",
      "Query Status: {'message': 'Getting offsets from FileStreamSource[file:/Users/sonika/Desktop/DS 2002/DS-2002/04-PySpark/lab_data/northwind/streaming/inventory_transactions]', 'isDataAvailable': False, 'isTriggerActive': True}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Query ID: {inventory_trans_bronze_query.id}\")\n",
    "print(f\"Query Name: {inventory_trans_bronze_query.name}\")\n",
    "print(f\"Query Status: {inventory_trans_bronze_query.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "095d3f5e-c44e-47f2-9f59-cffcd37ad64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory_trans_bronze_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8794a1-1b49-4c01-a87c-8b84cd79c30e",
   "metadata": {},
   "source": [
    "#### 7.3. Create the Silver Layer: Integrate \"Cold-path\" Data & Make Transformations\n",
    "##### 7.3.1. Prepare Role-Playing Dimension Primary and Business Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "28ce86a8-af37-487c-8b5b-7e70b50cd550",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Copy df_dim_date and rename 'date_key' and 'full_date' columns.\n",
    "df_dim_created_date = df_dim_date.select(col(\"date_key\").alias(\"created_date_key\"), col(\"full_date\").alias(\"created_full_date\"))\n",
    "\n",
    "#TODO: Copy df_dim_date and rename 'date_key' and 'full_date' columns.\n",
    "df_dim_modified_date = df_dim_date.select(col(\"date_key\").alias(\"modified_date_key\"), col(\"full_date\").alias(\"modified_full_date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "18206092-3f8e-41d0-8479-c7f18a95258b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(product_key=1, product_id=1, supplier_ids='4', product_code='NWTB-1', product_name='Northwind Traders Chai', standard_cost=Decimal('13.5000'), list_price=Decimal('18.0000'), reorder_level=10, target_level=40, quantity_per_unit='10 boxes x 20 bags', discontinued=False, minimum_reorder_quantity=10, category='Beverages'),\n",
       " Row(product_key=2, product_id=3, supplier_ids='10', product_code='NWTCO-3', product_name='Northwind Traders Syrup', standard_cost=Decimal('7.5000'), list_price=Decimal('10.0000'), reorder_level=25, target_level=100, quantity_per_unit='12 - 550 ml bottles', discontinued=False, minimum_reorder_quantity=25, category='Condiments')]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dim_products.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f7397e-1b73-4ca3-b65c-8fa92063676b",
   "metadata": {},
   "source": [
    "##### 7.3.2. Define Silver Query to Join Streaming with Batch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2bb0721a-016e-4b46-af06-e63b26fc0629",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inventory_trans_silver = spark.readStream.format(\"parquet\").load(inventory_trans_output_bronze) \\\n",
    "    .join(df_dim_products, \"product_id\") \\\n",
    "    .join(df_dim_created_date, df_dim_created_date.created_full_date.cast(DateType()) == col(\"created_full_date\").cast(DateType()), \"left_outer\") \\\n",
    "    .join(df_dim_modified_date, df_dim_modified_date.modified_full_date.cast(DateType()) == col(\"modified_full_date\").cast(DateType()), \"left_outer\") \\\n",
    "    .select(col(\"product_id\").cast(LongType()), \\\n",
    "            df_dim_created_date.created_date_key.cast(LongType()), \\\n",
    "            df_dim_modified_date.modified_date_key.cast(LongType()), \\\n",
    "            col(\"product_code\"), \\\n",
    "            col(\"list_price\"), \\\n",
    "            col(\"quantity_per_unit\"), \\\n",
    "            col(\"discontinued\") \\\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "62550a69-a8b1-40ea-8f99-9e2aeb864d2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_inventory_trans_silver.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6e4d7610-93db-4864-99ce-b2eb6a3ff27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: long (nullable = true)\n",
      " |-- created_date_key: long (nullable = true)\n",
      " |-- modified_date_key: long (nullable = true)\n",
      " |-- product_code: string (nullable = true)\n",
      " |-- list_price: decimal(19,4) (nullable = true)\n",
      " |-- quantity_per_unit: string (nullable = true)\n",
      " |-- discontinued: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_inventory_trans_silver.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8748cf85-83f5-437c-8888-63cdbc82dfce",
   "metadata": {},
   "source": [
    "##### 7.3.3. Write the Transformed Streaming data to the Data Lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "264ecda0-0f45-41fe-98ad-1f183034743e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory_trans_checkpoint_silver = os.path.join(inventory_trans_output_silver, '_checkpoint')\n",
    "\n",
    "inventory_trans_silver_query = (\n",
    "    df_inventory_trans_silver.writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .queryName(\"inventory_trans_silver\") \\\n",
    "    .trigger(availableNow = True) \\\n",
    "    .option(\"checkpointLocation\", inventory_trans_checkpoint_silver) \\\n",
    "    .option(\"compression\", \"snappy\") \\\n",
    "    .start(inventory_trans_output_silver)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9465503-b7e8-45bd-9f7a-eff4487470ed",
   "metadata": {},
   "source": [
    "##### 7.3.4. Unit Test: Implement Query Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ff5d0584-5ac9-4ba8-adbd-9a240313460c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: 9d19a1c9-3fba-488f-b870-ea3d696019e6\n",
      "Query Name: inventory_trans_silver\n",
      "Query Status: {'message': 'Getting offsets from FileStreamSource[file:/Users/sonika/Desktop/DS 2002/DS-2002/04-PySpark/spark-warehouse/northwind_dlh.db/fact_inventory_transactions/bronze]', 'isDataAvailable': False, 'isTriggerActive': True}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Query ID: {inventory_trans_silver_query.id}\")\n",
    "print(f\"Query Name: {inventory_trans_silver_query.name}\")\n",
    "print(f\"Query Status: {inventory_trans_silver_query.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef4e324-0c76-4cab-9eb2-e87148ba92b8",
   "metadata": {},
   "source": [
    "#### 7.4. Create Gold Layer: Perform Aggregations\n",
    "##### 7.4.1. Define a Query to Create a Business Report\n",
    "Create a new Gold table using the PySpark API. The table should include the total quantity (total quantity) of the inventory transactions placed per Product. Include the Inventory Transaction Type, and the Product Name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "659f7927-50a2-46c2-9b5e-7081e1f3e8eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNABLE_TO_INFER_SCHEMA] Unable to infer schema for Parquet. It must be specified manually.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[127], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_fact_inventory_trans_by_product_gold \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mreadStream\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mload(inventory_trans_output_silver) \\\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;241m.\u001b[39mjoin(df_dim_products, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproduct_key\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;241m.\u001b[39mjoin(df_dim_date, df_dim_date\u001b[38;5;241m.\u001b[39mdate_key\u001b[38;5;241m.\u001b[39mcast(IntegerType()) \u001b[38;5;241m==\u001b[39m col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreated_date_key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcast(IntegerType())) \\\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39mgroupBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalendar_quarter\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransaction_type\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproduct_name\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39magg(count(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantity\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_quantity\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \\\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39morderBy(asc(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_quantity\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pysparkenv/lib/python3.12/site-packages/pyspark/sql/streaming/readwriter.py:302\u001b[0m, in \u001b[0;36mDataStreamReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mstr\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(path\u001b[38;5;241m.\u001b[39mstrip()) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    298\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[1;32m    299\u001b[0m             error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVALUE_NOT_NON_EMPTY_STR\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    300\u001b[0m             message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_value\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(path)},\n\u001b[1;32m    301\u001b[0m         )\n\u001b[0;32m--> 302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mload(path))\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mload())\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pysparkenv/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pysparkenv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNABLE_TO_INFER_SCHEMA] Unable to infer schema for Parquet. It must be specified manually."
     ]
    }
   ],
   "source": [
    "df_fact_inventory_trans_by_product_gold = spark.readStream.format(\"parquet\").load(inventory_trans_output_silver) \\\n",
    "    .join(df_dim_products, \"product_key\") \\\n",
    "    .join(df_dim_date, df_dim_date.date_key.cast(IntegerType()) == col(\"created_date_key\").cast(IntegerType())) \\\n",
    "    .groupBy(\"calendar_quarter\", \"transaction_type\", \"product_name\") \\\n",
    "    .agg(count(\"quantity\").alias(\"total_quantity\")) \\\n",
    "    .orderBy(asc(\"total_quantity\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf05ceb-f4d6-44d0-b8e9-600a99b15c2d",
   "metadata": {},
   "source": [
    "##### 7.4.2. Write the Streaming data to Memory in \"Complete\" mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3d116c-dc4a-42e9-bb0f-4cb7ba4f648f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory_trans_gold_query = (\n",
    "    df_fact_inventory_trans_by_product_gold.writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .queryName(\"fact_inventory_trans_by_product\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2067cb-8161-431a-aa44-a64aeb4edbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "wait_until_stream_is_ready(inventory_trans_gold_query, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400a5435-0180-4155-a4db-3d5ded849b13",
   "metadata": {},
   "source": [
    "##### 7.4.3. Query the Gold Data from Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93253cef-fb6a-4fa4-b622-db2ff1a13676",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact_inventory_trans_by_product = spark.sql(\"SELECT * FROM fact_inventory_trans_by_product\")\n",
    "df_fact_inventory_trans_by_product.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054ebf94-58fe-4851-9399-c1f41b80c556",
   "metadata": {},
   "source": [
    "##### 7.4.4 Create the Final Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4582066-5790-4931-beb7-c90f03ded4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact_inventory_trans_by_product_gold_final = df_fact_inventory_trans_by_product \\\n",
    "    .select(col(\"calendar_quarter\").alias(\"Quarter Created\"), \\\n",
    "            col(\"transaction_type\").alias(\"Transaction\"), \\\n",
    "            col(\"product_name\").alias(\"Product\"), \\\n",
    "            col(\"quantity\").alias(\"Total Quantity\"), \\\n",
    "    .orderBy(asc(\"Total Quantity\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dfd0b7-bba2-40f5-8efd-b8b47961da38",
   "metadata": {},
   "source": [
    "##### 7.4.5. Load the Final Results into a New Table and Display the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252eec1c-abe8-439f-bc4c-2ec181a48baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact_inventory_trans_by_product_gold_final.write.saveAsTable(f\"{dest_database}.fact_inventory_trans_by_product\", mode=\"overwrite\")\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.fact_inventory_trans_by_product\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b2dcb2-82e3-4a30-b04d-b06d92044735",
   "metadata": {},
   "source": [
    "### 8.0. Use PySpark Structured Streaming to Process (Hot Path) <span style=\"color:darkred\">Purchase Orders</span> Fact Data\n",
    "#### 8.1. Verify the location of the source data files on the file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b074ee6-767a-426e-9ff0-466f50f2ed64",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_file_info(purchase_orders_stream_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1743fafa-155b-46be-b28a-1d4b31dd44e3",
   "metadata": {},
   "source": [
    "#### 8.2. Create the Bronze Layer: Stage <span style=\"color:darkred\">Purchase Orders Fact table</span> Data\n",
    "##### 8.2.1. Read \"Raw\" JSON file data into a Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4945ea6-4215-4ccf-9d49-628f3848fbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_purchase_orders_bronze = (\n",
    "    spark.readStream \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(purchase_orders_stream_dir)\n",
    ")\n",
    "\n",
    "df_purchase_orders_bronze.isStreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e5f3a9-fcc2-485c-96b8-c62ef8bee2cb",
   "metadata": {},
   "source": [
    "##### 8.2.2. Write the Streaming Data to a Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740a81c5-ffb0-4158-8644-67b1e58c55da",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_orders_checkpoint_bronze = os.path.join(purchase_orders_output_bronze, '_checkpoint')\n",
    "\n",
    "purchase_orders_bronze_query = (\n",
    "    df_purchase_orders_bronze\n",
    "    # TODO: Add Current Timestamp and Input Filename columns for Traceability\n",
    "    .withColumn(\"receipt_time\", current_timestamp())\n",
    "    .withColumn(\"source_file\", input_file_name())\n",
    "    \n",
    "    .writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .queryName(\"purchase_orders_bronze\")\n",
    "    .trigger(availableNow = True) \\\n",
    "    .option(\"checkpointLocation\", purchase_orders_checkpoint_bronze) \\\n",
    "    .option(\"compression\", \"snappy\") \\\n",
    "    .start(purchase_orders_output_bronze)\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be7eecc-e366-4c42-8ce2-139124e43fbf",
   "metadata": {},
   "source": [
    "##### 8.2.3. Unit Test: Implement Query Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd8e3da-70f4-4132-942e-6b494cd48998",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Query ID: {purchase_orders_bronze_query.id}\")\n",
    "print(f\"Query Name: {purchase_orders_bronze_query.name}\")\n",
    "print(f\"Query Status: {purchase_orders_bronze_query.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac5d2ab-d1fd-4900-a14a-c0605ac58f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_orders_bronze_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c242a939-e121-4d6c-83a5-70817bc6d1b8",
   "metadata": {},
   "source": [
    "#### 8.3. Create the Silver Layer: Integrate \"Cold-path\" Data & Make Transformations\n",
    "##### 8.3.1. Prepare Role-Playing Dimension Primary and Business Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74d5b82-da69-471f-a362-21940382adc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Copy 'df_dim_employees' and rename 'employee_key' and 'employee_id' columns.\n",
    "df_dim_created_by = df_dim_employees.select(col(\"employee_key\").alias(\"created_by_employee_key\"), col(\"employee_id\").alias(\"created_by_employee_id\"))\n",
    "df_dim_approved_by = df_dim_employees.select(col(\"employee_key\").alias(\"approved_by_employee_key\"), col(\"employee_id\").alias(\"approved_by_employee_id\"))\n",
    "df_dim_submitted_by = df_dim_employees.select(col(\"employee_key\").alias(\"submitted_by_employee_key\"), col(\"employee_id\").alias(\"submitted_by_employee_id\"))\n",
    "\n",
    "#TODO: Copy df_dim_date and rename 'date_key' and 'full_date' columns.\n",
    "df_dim_submitted_date = df_dim_date.select(col(\"date_key\").alias(\"submitted_date_key\"), col(\"full_date\").alias(\"submitted_full_date\"))\n",
    "df_dim_creation_date = df_dim_date.select(col(\"date_key\").alias(\"creation_date_key\"), col(\"full_date\").alias(\"creation_full_date\"))\n",
    "df_dim_approved_date = df_dim_date.select(col(\"date_key\").alias(\"approved_date_key\"), col(\"full_date\").alias(\"approved_full_date\"))\n",
    "df_dim_date_received = df_dim_date.select(col(\"date_key\").alias(\"received_date_key\"), col(\"full_date\").alias(\"received_full_date\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9a6681-5eb1-4c45-89b2-fd125a9eb6a0",
   "metadata": {},
   "source": [
    "##### 8.3.2. Define Silver Query to Join Streaming with Batch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a807bc-186d-44c0-b181-01c2f6c6e1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_purchase_orders_silver = spark.readStream.format(\"parquet\").load(purchase_orders_output_bronze) \\\n",
    "    .join(df_dim_products, \"product_id\") \\\n",
    "    .join(df_dim_suppliers, \"supplier_id\") \\\n",
    "    .join(df_dim_created_by, df_dim_created_by.employee_no == col(\"created_by_employee_id\").cast(IntegerType()), \"left_outer\") \\\n",
    "    .join(df_dim_approved_by, df_dim_approved_by.employee_no == col(\"approved_by_employee_id\").cast(IntegerType()), \"left_outer\") \\\n",
    "    .join(df_dim_submitted_by, df_dim_submitted_by.employee_no == col(\"submitted_by_employee_id\").cast(IntegerType()), \"left_outer\") \\\n",
    "    \n",
    "    df_dim_submitted_date.submitted_date_key.cast(LongType()), \\\n",
    "    df_dim_creation_date.creation_date_key.cast(LongType()), \\\n",
    "    .join(df_dim_approved_date, df_dim_approved_date.approved_by_full_date.cast(DateType()) == col(\"approved_full_date\").cast(DateType()), \"left_outer\") \\\n",
    "    .join(df_dim_date_received, df_dim_date_received.received_by_full_date.cast(DateType()) == col(\"received_full_date\").cast(DateType()), \"left_outer\") \\\n",
    "    # .select() the appropriate columns from the 'purchase orders bronze' stream\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ffacdc-813d-49db-bc17-5d510371545b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_purchase_orders_silver.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7daf6d-3121-4dd7-9e01-4b51397b69cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_purchase_orders_silver.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b891abcc-4c45-4364-bd6d-30a0a7d886e6",
   "metadata": {},
   "source": [
    "##### 8.3.3. Write the Transformed Streaming data to the Data Lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b414de3c-ab4d-4d4f-a987-81fb417c99a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_orders_checkpoint_silver = os.path.join(purchase_orders_output_silver, '_checkpoint')\n",
    "\n",
    "purchase_orders_silver_query = (\n",
    "    df_purchase_orders_silver.writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .queryName(\"purchase_orders_silver\")\n",
    "    .trigger(availableNow = True) \\\n",
    "    .option(\"checkpointLocation\", purchase_orders_checkpoint_silver) \\\n",
    "    .option(\"compression\", \"snappy\") \\\n",
    "    .start(purchase_orders_output_silver)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2133b8c8-59a9-4f0b-a47a-4eb5b6b64c7b",
   "metadata": {},
   "source": [
    "##### 8.3.4. Unit Test: Implement Query Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6eb141-c92e-4f70-9757-0f7e93eac435",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Query ID: {purchase_orders_silver_query.id}\")\n",
    "print(f\"Query Name: {purchase_orders_silver_query.name}\")\n",
    "print(f\"Query Status: {purchase_orders_silver_query.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89be4f13-e58e-40dd-8286-a94c195747fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_orders_silver_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e153434-28ad-47f1-889c-d3c5ad6e9825",
   "metadata": {},
   "source": [
    "#### 8.4. Create Gold Layer: Perform Aggregations\n",
    "##### 8.4.1. Define a Query to Create a Business Report\n",
    "Create a new Gold table using the PySpark API. The table should include the Suppliers' Company Name, the Product Name, the Total Quantity, Total Unit Cost, and Total List Price for all the purchase orders placed per Supplier for each Product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782a3b8d-7be1-457e-b2b2-0858269a886f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact_pos_products_per_supplier_gold = spark.readStream.format(\"parquet\").load(purchase_orders_output_silver) \\\n",
    "    .join(df_products, \"product_key\") \\\n",
    "    .join(df_dim_suppliers, \"supplier_key\") \\\n",
    "    .groupBy(\"company\", \"product_name\") \\\n",
    "    .agg(count(\"po_detail_quantity\").alias(\"Total Quantity\")) \\\n",
    "    .agg(count(\"po_detail_unit_cost\").alias(\"Unit Cost\")) \\\n",
    "    .agg(count(\"list_price\").alias(\"Total List Price\")) \\\n",
    ".orderBy(desc(\"Total Quantity\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c972e0-8ac6-458f-b6ab-703348f0b887",
   "metadata": {},
   "source": [
    "##### 8.4.2. Write the Streaming data to Memory in \"Complete\" mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2a4ee8-54a1-4f20-b100-0116be5c1bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_orders_gold_query = (\n",
    "    df_fact_pos_products_per_supplier_gold.writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .queryName(\"fact_pos_products_per_supplier\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fe0ce1-73ec-4233-9246-8663086fa37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wait_until_stream_is_ready(purchase_orders_gold_query, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8969d4cc-85e2-4ddc-bae2-875243fbcd98",
   "metadata": {},
   "source": [
    "##### 8.4.3. Query the Gold Data from Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8848aa-adb0-40e6-8c68-de8b2936e350",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact_pos_products_per_supplier = spark.sql(\"SELECT * FROM fact_pos_products_per_supplier\")\n",
    "df_fact_pos_products_per_supplier.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5babafb4-e123-41ff-8dbc-62b29fe58dd6",
   "metadata": {},
   "source": [
    "##### 8.4.4. Create the Final Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb16afc-a048-44b0-b8e3-6a73d677723b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact_pos_products_per_supplier_gold_final = df_fact_pos_products_per_supplier \\\n",
    "# .select() the 'company' column as 'Supplier', the 'product_name' column as 'Product',\n",
    "# along with the 'Total Quantity', 'Total Unit Cost', and 'Total List Price' columns\n",
    ".select(col(\"company\").alias(\"Supplier\"), \\\n",
    "        col(\"product_name\").alias(\"Product\"), \\\n",
    "        col(\"Total Quantity\")), \\\n",
    "        col(\"Total Unit Cost\")), \\\n",
    "        col(\"Total List Price\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2a3411-c649-4036-a28f-4fdc289c5814",
   "metadata": {},
   "source": [
    "##### 8.4.5. Load the Final Results into a New Table and Display the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45668f44-148b-418f-b956-7dbe2de36e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact_pos_products_per_supplier_gold_final.write.saveAsTable(f\"{dest_database}.fact_pos_products_per_supplier\", mode=\"overwrite\")\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.fact_pos_products_per_supplier\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efd14eb-1c30-4d29-b5ca-46dadafa5419",
   "metadata": {},
   "source": [
    "### 9.0. Stop the Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6edcbef-aa54-4ca0-aa9f-2b58c8931537",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
